{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2222579-9a7b-4f27-8ec8-aec7e6efd80a",
   "metadata": {
    "id": "a2222579-9a7b-4f27-8ec8-aec7e6efd80a"
   },
   "source": [
    "## Imports and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b77b39f-323f-45f9-8f03-f06c2f55ad10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3b77b39f-323f-45f9-8f03-f06c2f55ad10",
    "outputId": "9bfbf64f-4026-4a8d-8ff8-ac85c816ccb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /Users/qmy/opt/miniconda3/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /Users/qmy/opt/miniconda3/lib/python3.10/site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /Users/qmy/opt/miniconda3/lib/python3.10/site-packages (from rouge_score) (3.7)\n",
      "Requirement already satisfied: numpy in /Users/qmy/opt/miniconda3/lib/python3.10/site-packages (from rouge_score) (1.23.3)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/qmy/opt/miniconda3/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/qmy/opt/miniconda3/lib/python3.10/site-packages (from nltk->rouge_score) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/qmy/opt/miniconda3/lib/python3.10/site-packages (from nltk->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/qmy/opt/miniconda3/lib/python3.10/site-packages (from nltk->rouge_score) (2022.9.13)\n",
      "Requirement already satisfied: tqdm in /Users/qmy/opt/miniconda3/lib/python3.10/site-packages (from nltk->rouge_score) (4.64.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qmy/opt/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ! pip install transformers\n",
    "# ! pip3 install wandb\n",
    "! pip install rouge_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoTokenizer, LongformerTokenizer, RobertaTokenizer, LongformerModel\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "# from transformers import LongformerModel\n",
    "\n",
    "from torch import cuda, nn, optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import rouge_score\n",
    "# import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25f49cd5-8dc0-4c85-bdc5-83d71e7371cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25f49cd5-8dc0-4c85-bdc5-83d71e7371cd",
    "outputId": "898ece32-861f-4498-e814-139624c153ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "manual_seed = 595\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc84a4d-7204-4466-95ee-9907cbba2107",
   "metadata": {
    "id": "dbc84a4d-7204-4466-95ee-9907cbba2107"
   },
   "source": [
    "## Read the Cleaned Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f48d661-5ec8-4851-b510-c9ef877d8d38",
   "metadata": {
    "id": "8f48d661-5ec8-4851-b510-c9ef877d8d38"
   },
   "source": [
    "### Define the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f955f6-045d-4453-ad45-39e67216ef59",
   "metadata": {
    "id": "08f955f6-045d-4453-ad45-39e67216ef59"
   },
   "outputs": [],
   "source": [
    "# run locally\n",
    "text_path = '../formatted_cases/'\n",
    "file = '../../annotated_data.xlsx'\n",
    "REGEX = r';+'\n",
    "sup_path = '../annotated_sup/'\n",
    "multi_path = text_path + 'multiple_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46377399-e3a0-4a2a-9325-d86d5f0b6939",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46377399-e3a0-4a2a-9325-d86d5f0b6939",
    "outputId": "2cae9fb7-e1bd-4aa2-81aa-20cbbfa6ef88"
   },
   "outputs": [],
   "source": [
    "# # run on Google Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# text_path = '/content/gdrive/My Drive/595/formatted_cases/'\n",
    "# file = '/content/gdrive/My Drive/595/annotated_data.xlsx'\n",
    "# REGEX = r';+'\n",
    "# sup_path = '/content/gdrive/My Drive/595/annotated_sup/'\n",
    "# multi_path = text_path + 'multiple_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "kmLgbrAaYPLO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "id": "kmLgbrAaYPLO",
    "outputId": "259f57cd-c331-495a-962a-3473ddcb29e4"
   },
   "outputs": [],
   "source": [
    "# wandb.login()\n",
    "# wandb.init(project=\"RTB_Cases\", entity=\"qmygrace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5846c80-5bf1-4118-90d0-b7d9a2f5b4a5",
   "metadata": {
    "id": "f5846c80-5bf1-4118-90d0-b7d9a2f5b4a5"
   },
   "source": [
    "### Clean the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b773cee8-0f30-48b5-a78b-c8482bb76e49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 921
    },
    "id": "b773cee8-0f30-48b5-a78b-c8482bb76e49",
    "outputId": "cc4478bc-4bb3-4b72-cc4a-caca290ab3f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(702, 50)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>What is the file number of the case?</th>\n",
       "      <th>What was the date of the hearing? [mm/dd/yyyy]</th>\n",
       "      <th>What was the date of the decision? [mm/dd/yyyy]</th>\n",
       "      <th>Who was the member adjudicating the decision?</th>\n",
       "      <th>What was the location of the landlord tenant board?</th>\n",
       "      <th>Did the decision state the landlord was represented?</th>\n",
       "      <th>Did the decision state the landlord attended the hearing?</th>\n",
       "      <th>Did the decision state the tenant was represented?</th>\n",
       "      <th>Did the decision state the tenant attended the hearing?</th>\n",
       "      <th>Did the decision state the landlord was a not-for-profit landlord (e.g. Toronto Community Housing)?</th>\n",
       "      <th>...</th>\n",
       "      <th>If the tenant did propose a payment plan, did the member accept the proposed payment plan?</th>\n",
       "      <th>If a payment plan was ordered, what was the length of the payment plan?</th>\n",
       "      <th>Did the decision mention the tenant’s difficulty finding alternative housing for any reason e.g.physical limitations, reliance on social assistance, etc.?</th>\n",
       "      <th>If the tenant had difficulty finding alternative housing for any reason, which of the following were applicable to the tenant?</th>\n",
       "      <th>Did the decision state the tenant was given prior notice for the eviction?</th>\n",
       "      <th>If the tenant was given prior notice for the eviction, how much notice was given?</th>\n",
       "      <th>Did the decisions state postponement would result in the tenant accruing additional arrears?</th>\n",
       "      <th>Which other specific applications of the landlord or the tenant were mentioned?</th>\n",
       "      <th>Did the decision mention the validity of an N4 eviction notice?</th>\n",
       "      <th>Were there detail(s) in the decision not captured by this questionnaire that should be included?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CEL-87788-19</td>\n",
       "      <td>2019-10-16 00:00:00</td>\n",
       "      <td>2020-06-04 00:00:00</td>\n",
       "      <td>Sonia Anwar-Ali</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>12</td>\n",
       "      <td>No</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>L2: Application to End a Tenancy and Evict a T...</td>\n",
       "      <td>No</td>\n",
       "      <td>Tenant was a single mother with no support fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CEL-90549-19</td>\n",
       "      <td>2020-01-22 00:00:00</td>\n",
       "      <td>2020-01-10 00:00:00</td>\n",
       "      <td>Shelby Whittick</td>\n",
       "      <td>Mississauga</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No other specific applications were mentioned</td>\n",
       "      <td>No</td>\n",
       "      <td>Not stated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEL-94478-18</td>\n",
       "      <td>2018-10-31 00:00:00</td>\n",
       "      <td>2018-11-21 00:00:00</td>\n",
       "      <td>Ruth Carey (Vice Chair)</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>N13: Notice to End your Tenancy Because the La...</td>\n",
       "      <td>No</td>\n",
       "      <td>Previous decision TEL-92736-18 &lt; This decision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEL-94493-18</td>\n",
       "      <td>2018-10-31 00:00:00</td>\n",
       "      <td>2018-11-21 00:00:00</td>\n",
       "      <td>Ruth Carey (Vice Chair)</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>No other specific applications were mentioned</td>\n",
       "      <td>No</td>\n",
       "      <td>There were 7 previous application for non-paym...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CEL-72994-18</td>\n",
       "      <td>2018-03-07 00:00:00</td>\n",
       "      <td>2018-03-14 00:00:00</td>\n",
       "      <td>Avril Cardoso</td>\n",
       "      <td>Mississauga</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>No other specific applications were mentioned</td>\n",
       "      <td>No</td>\n",
       "      <td>Third Application by Landlord in past 6 months...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CEL-73021-18</td>\n",
       "      <td>2018-06-15 00:00:00</td>\n",
       "      <td>2018-06-18 00:00:00</td>\n",
       "      <td>Avril Cardoso</td>\n",
       "      <td>Mississauga</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>L1: Application to Evict a Tenant for Non-paym...</td>\n",
       "      <td>No</td>\n",
       "      <td>Tenant did not show up because hearing took pl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  What is the file number of the case?  \\\n",
       "0                         CEL-87788-19   \n",
       "1                         CEL-90549-19   \n",
       "2                         TEL-94478-18   \n",
       "3                         TEL-94493-18   \n",
       "4                         CEL-72994-18   \n",
       "5                         CEL-73021-18   \n",
       "\n",
       "  What was the date of the hearing? [mm/dd/yyyy]  \\\n",
       "0                            2019-10-16 00:00:00   \n",
       "1                            2020-01-22 00:00:00   \n",
       "2                            2018-10-31 00:00:00   \n",
       "3                            2018-10-31 00:00:00   \n",
       "4                            2018-03-07 00:00:00   \n",
       "5                            2018-06-15 00:00:00   \n",
       "\n",
       "  What was the date of the decision? [mm/dd/yyyy]  \\\n",
       "0                             2020-06-04 00:00:00   \n",
       "1                             2020-01-10 00:00:00   \n",
       "2                             2018-11-21 00:00:00   \n",
       "3                             2018-11-21 00:00:00   \n",
       "4                             2018-03-14 00:00:00   \n",
       "5                             2018-06-18 00:00:00   \n",
       "\n",
       "  Who was the member adjudicating the decision?  \\\n",
       "0                               Sonia Anwar-Ali   \n",
       "1                               Shelby Whittick   \n",
       "2                       Ruth Carey (Vice Chair)   \n",
       "3                      Ruth Carey (Vice Chair)    \n",
       "4                                 Avril Cardoso   \n",
       "5                                 Avril Cardoso   \n",
       "\n",
       "  What was the location of the landlord tenant board?  \\\n",
       "0                                            Toronto    \n",
       "1                                        Mississauga    \n",
       "2                                            Toronto    \n",
       "3                                            Toronto    \n",
       "4                                        Mississauga    \n",
       "5                                        Mississauga    \n",
       "\n",
       "  Did the decision state the landlord was represented?  \\\n",
       "0                                                Yes     \n",
       "1                                                Yes     \n",
       "2                                                Yes     \n",
       "3                                                Yes     \n",
       "4                                                Yes     \n",
       "5                                                Yes     \n",
       "\n",
       "  Did the decision state the landlord attended the hearing?  \\\n",
       "0                                         Not stated          \n",
       "1                                                Yes          \n",
       "2                                                Yes          \n",
       "3                                                Yes          \n",
       "4                                                 No          \n",
       "5                                                 No          \n",
       "\n",
       "  Did the decision state the tenant was represented?  \\\n",
       "0                                                 No   \n",
       "1                                                 No   \n",
       "2                                                 No   \n",
       "3                                                 No   \n",
       "4                                                Yes   \n",
       "5                                                 No   \n",
       "\n",
       "  Did the decision state the tenant attended the hearing?  \\\n",
       "0                                         Not stated        \n",
       "1                                                Yes        \n",
       "2                                                Yes        \n",
       "3                                                Yes        \n",
       "4                                                 No        \n",
       "5                                                 No        \n",
       "\n",
       "  Did the decision state the landlord was a not-for-profit landlord (e.g. Toronto Community Housing)?  \\\n",
       "0                                                 No                                                    \n",
       "1                                                 No                                                    \n",
       "2                                                 No                                                    \n",
       "3                                                 No                                                    \n",
       "4                                                 No                                                    \n",
       "5                                                 No                                                    \n",
       "\n",
       "   ...  \\\n",
       "0  ...   \n",
       "1  ...   \n",
       "2  ...   \n",
       "3  ...   \n",
       "4  ...   \n",
       "5  ...   \n",
       "\n",
       "  If the tenant did propose a payment plan, did the member accept the proposed payment plan?  \\\n",
       "0                                         Not stated                                           \n",
       "1                                                 No                                           \n",
       "2                                         Not stated                                           \n",
       "3                                                Yes                                           \n",
       "4                                                 No                                           \n",
       "5                                         Not stated                                           \n",
       "\n",
       "  If a payment plan was ordered, what was the length of the payment plan?   \\\n",
       "0                                                 12                         \n",
       "1                                         Not stated                         \n",
       "2                                         Not stated                         \n",
       "3                                                  1                         \n",
       "4                                         Not stated                         \n",
       "5                                         Not stated                         \n",
       "\n",
       "  Did the decision mention the tenant’s difficulty finding alternative housing for any reason e.g.physical limitations, reliance on social assistance, etc.?  \\\n",
       "0                                                 No                                                                                                           \n",
       "1                                                 No                                                                                                           \n",
       "2                                                 No                                                                                                           \n",
       "3                                                 No                                                                                                           \n",
       "4                                                 No                                                                                                           \n",
       "5                                                 No                                                                                                           \n",
       "\n",
       "  If the tenant had difficulty finding alternative housing for any reason, which of the following were applicable to the tenant?  \\\n",
       "0                                         Not stated                                                                               \n",
       "1                                         Not stated                                                                               \n",
       "2                                         Not stated                                                                               \n",
       "3                                         Not stated                                                                               \n",
       "4                                         Not stated                                                                               \n",
       "5                                         Not stated                                                                               \n",
       "\n",
       "  Did the decision state the tenant was given prior notice for the eviction?  \\\n",
       "0                                                 No                           \n",
       "1                                                Yes                           \n",
       "2                                                Yes                           \n",
       "3                                                Yes                           \n",
       "4                                                Yes                           \n",
       "5                                                Yes                           \n",
       "\n",
       "  If the tenant was given prior notice for the eviction, how much notice was given?  \\\n",
       "0                                         Not stated                                  \n",
       "1                                         Not stated                                  \n",
       "2                                         Not stated                                  \n",
       "3                                         Not stated                                  \n",
       "4                                         Not stated                                  \n",
       "5                                         Not stated                                  \n",
       "\n",
       "  Did the decisions state postponement would result in the tenant accruing additional arrears?  \\\n",
       "0                                                 No                                             \n",
       "1                                                Yes                                             \n",
       "2                                                 No                                             \n",
       "3                                                 No                                             \n",
       "4                                                 No                                             \n",
       "5                                                 No                                             \n",
       "\n",
       "  Which other specific applications of the landlord or the tenant were mentioned?  \\\n",
       "0  L2: Application to End a Tenancy and Evict a T...                                \n",
       "1      No other specific applications were mentioned                                \n",
       "2  N13: Notice to End your Tenancy Because the La...                                \n",
       "3      No other specific applications were mentioned                                \n",
       "4      No other specific applications were mentioned                                \n",
       "5  L1: Application to Evict a Tenant for Non-paym...                                \n",
       "\n",
       "  Did the decision mention the validity of an N4 eviction notice?  \\\n",
       "0                                                 No                \n",
       "1                                                 No                \n",
       "2                                                 No                \n",
       "3                                                 No                \n",
       "4                                                 No                \n",
       "5                                                 No                \n",
       "\n",
       "  Were there detail(s) in the decision not captured by this questionnaire that should be included?  \n",
       "0  Tenant was a single mother with no support fro...                                                \n",
       "1                                         Not stated                                                \n",
       "2  Previous decision TEL-92736-18 < This decision...                                                \n",
       "3  There were 7 previous application for non-paym...                                                \n",
       "4  Third Application by Landlord in past 6 months...                                                \n",
       "5  Tenant did not show up because hearing took pl...                                                \n",
       "\n",
       "[6 rows x 50 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(file)\n",
    "df['What is the file number of the case?'] = df['What is the file number of the case?'].str.replace(' and ', ';')\n",
    "df['What is the file number of the case?'] = df['What is the file number of the case?'].str.replace(' ', ';')\n",
    "df['What is the file number of the case?'] = df['What is the file number of the case?'].str.replace('/', ';')\n",
    "df['What is the file number of the case?'] = df['What is the file number of the case?'].str.strip(';')\n",
    "df['What is the file number of the case?'] = df['What is the file number of the case?'].apply(lambda x: re.sub(REGEX, ';', x))\n",
    "df['What is the file number of the case?'] = df['What is the file number of the case?'].str.replace('File;number:;', '')\n",
    "df['What is the file number of the case?'] = df['What is the file number of the case?'].str.replace('TET-89650-18;TET-89650-18', 'TET-89650-18;TEL-90138-18')\n",
    "df = df.fillna('Not stated')\n",
    "df = df.replace('Not applicable', 'Not stated')\n",
    "df.rename(columns={\n",
    "    'If yes to the previous question, did the decision state these conditions would make moving particularly burdensome?':\n",
    "    'If any of the children had mental, medical or physical conditions, did the decision state these conditions would make moving particularly burdensome?',\n",
    "    'If yes to the previous question, which of the following were applicable to the tenant?':\n",
    "    'If the tenant had difficulty finding alternative housing for any reason, which of the following were applicable to the tenant?'    \n",
    "}, inplace=True)\n",
    "\n",
    "df = df.iloc[:, 2:-2]\n",
    "\n",
    "print(df.shape)\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c843c20-c7ea-495a-a031-e436f556d200",
   "metadata": {
    "id": "3c843c20-c7ea-495a-a031-e436f556d200"
   },
   "outputs": [],
   "source": [
    "# df.columns   #`Timestamp` is not the time of the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8789c210-18b6-46d6-a990-29fce2add302",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 921
    },
    "id": "8789c210-18b6-46d6-a990-29fce2add302",
    "outputId": "bd949f31-af78-48a4-c6ac-eb0b5ba1a276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(682, 50)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>What is the file number of the case?</th>\n",
       "      <th>What was the date of the hearing? [mm/dd/yyyy]</th>\n",
       "      <th>What was the date of the decision? [mm/dd/yyyy]</th>\n",
       "      <th>Who was the member adjudicating the decision?</th>\n",
       "      <th>What was the location of the landlord tenant board?</th>\n",
       "      <th>Did the decision state the landlord was represented?</th>\n",
       "      <th>Did the decision state the landlord attended the hearing?</th>\n",
       "      <th>Did the decision state the tenant was represented?</th>\n",
       "      <th>Did the decision state the tenant attended the hearing?</th>\n",
       "      <th>Did the decision state the landlord was a not-for-profit landlord (e.g. Toronto Community Housing)?</th>\n",
       "      <th>...</th>\n",
       "      <th>If the tenant did propose a payment plan, did the member accept the proposed payment plan?</th>\n",
       "      <th>If a payment plan was ordered, what was the length of the payment plan?</th>\n",
       "      <th>Did the decision mention the tenant’s difficulty finding alternative housing for any reason e.g.physical limitations, reliance on social assistance, etc.?</th>\n",
       "      <th>If the tenant had difficulty finding alternative housing for any reason, which of the following were applicable to the tenant?</th>\n",
       "      <th>Did the decision state the tenant was given prior notice for the eviction?</th>\n",
       "      <th>If the tenant was given prior notice for the eviction, how much notice was given?</th>\n",
       "      <th>Did the decisions state postponement would result in the tenant accruing additional arrears?</th>\n",
       "      <th>Which other specific applications of the landlord or the tenant were mentioned?</th>\n",
       "      <th>Did the decision mention the validity of an N4 eviction notice?</th>\n",
       "      <th>Were there detail(s) in the decision not captured by this questionnaire that should be included?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CEL-87788-19</td>\n",
       "      <td>2019-10-16 00:00:00</td>\n",
       "      <td>2020-06-04 00:00:00</td>\n",
       "      <td>Sonia Anwar-Ali</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>12</td>\n",
       "      <td>No</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>L2: Application to End a Tenancy and Evict a T...</td>\n",
       "      <td>No</td>\n",
       "      <td>Tenant was a single mother with no support fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CEL-90549-19</td>\n",
       "      <td>2020-01-22 00:00:00</td>\n",
       "      <td>2020-01-10 00:00:00</td>\n",
       "      <td>Shelby Whittick</td>\n",
       "      <td>Mississauga</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No other specific applications were mentioned</td>\n",
       "      <td>No</td>\n",
       "      <td>Not stated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEL-94478-18</td>\n",
       "      <td>2018-10-31 00:00:00</td>\n",
       "      <td>2018-11-21 00:00:00</td>\n",
       "      <td>Ruth Carey (Vice Chair)</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>N13: Notice to End your Tenancy Because the La...</td>\n",
       "      <td>No</td>\n",
       "      <td>Previous decision TEL-92736-18 &lt; This decision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEL-94493-18</td>\n",
       "      <td>2018-10-31 00:00:00</td>\n",
       "      <td>2018-11-21 00:00:00</td>\n",
       "      <td>Ruth Carey (Vice Chair)</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>No other specific applications were mentioned</td>\n",
       "      <td>No</td>\n",
       "      <td>There were 7 previous application for non-paym...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CEL-72994-18</td>\n",
       "      <td>2018-03-07 00:00:00</td>\n",
       "      <td>2018-03-14 00:00:00</td>\n",
       "      <td>Avril Cardoso</td>\n",
       "      <td>Mississauga</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>No other specific applications were mentioned</td>\n",
       "      <td>No</td>\n",
       "      <td>Third Application by Landlord in past 6 months...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CEL-73021-18</td>\n",
       "      <td>2018-06-15 00:00:00</td>\n",
       "      <td>2018-06-18 00:00:00</td>\n",
       "      <td>Avril Cardoso</td>\n",
       "      <td>Mississauga</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>L1: Application to Evict a Tenant for Non-paym...</td>\n",
       "      <td>No</td>\n",
       "      <td>Tenant did not show up because hearing took pl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  What is the file number of the case?  \\\n",
       "0                         CEL-87788-19   \n",
       "1                         CEL-90549-19   \n",
       "2                         TEL-94478-18   \n",
       "3                         TEL-94493-18   \n",
       "4                         CEL-72994-18   \n",
       "5                         CEL-73021-18   \n",
       "\n",
       "  What was the date of the hearing? [mm/dd/yyyy]  \\\n",
       "0                            2019-10-16 00:00:00   \n",
       "1                            2020-01-22 00:00:00   \n",
       "2                            2018-10-31 00:00:00   \n",
       "3                            2018-10-31 00:00:00   \n",
       "4                            2018-03-07 00:00:00   \n",
       "5                            2018-06-15 00:00:00   \n",
       "\n",
       "  What was the date of the decision? [mm/dd/yyyy]  \\\n",
       "0                             2020-06-04 00:00:00   \n",
       "1                             2020-01-10 00:00:00   \n",
       "2                             2018-11-21 00:00:00   \n",
       "3                             2018-11-21 00:00:00   \n",
       "4                             2018-03-14 00:00:00   \n",
       "5                             2018-06-18 00:00:00   \n",
       "\n",
       "  Who was the member adjudicating the decision?  \\\n",
       "0                               Sonia Anwar-Ali   \n",
       "1                               Shelby Whittick   \n",
       "2                       Ruth Carey (Vice Chair)   \n",
       "3                      Ruth Carey (Vice Chair)    \n",
       "4                                 Avril Cardoso   \n",
       "5                                 Avril Cardoso   \n",
       "\n",
       "  What was the location of the landlord tenant board?  \\\n",
       "0                                            Toronto    \n",
       "1                                        Mississauga    \n",
       "2                                            Toronto    \n",
       "3                                            Toronto    \n",
       "4                                        Mississauga    \n",
       "5                                        Mississauga    \n",
       "\n",
       "  Did the decision state the landlord was represented?  \\\n",
       "0                                                Yes     \n",
       "1                                                Yes     \n",
       "2                                                Yes     \n",
       "3                                                Yes     \n",
       "4                                                Yes     \n",
       "5                                                Yes     \n",
       "\n",
       "  Did the decision state the landlord attended the hearing?  \\\n",
       "0                                         Not stated          \n",
       "1                                                Yes          \n",
       "2                                                Yes          \n",
       "3                                                Yes          \n",
       "4                                                 No          \n",
       "5                                                 No          \n",
       "\n",
       "  Did the decision state the tenant was represented?  \\\n",
       "0                                                 No   \n",
       "1                                                 No   \n",
       "2                                                 No   \n",
       "3                                                 No   \n",
       "4                                                Yes   \n",
       "5                                                 No   \n",
       "\n",
       "  Did the decision state the tenant attended the hearing?  \\\n",
       "0                                         Not stated        \n",
       "1                                                Yes        \n",
       "2                                                Yes        \n",
       "3                                                Yes        \n",
       "4                                                 No        \n",
       "5                                                 No        \n",
       "\n",
       "  Did the decision state the landlord was a not-for-profit landlord (e.g. Toronto Community Housing)?  \\\n",
       "0                                                 No                                                    \n",
       "1                                                 No                                                    \n",
       "2                                                 No                                                    \n",
       "3                                                 No                                                    \n",
       "4                                                 No                                                    \n",
       "5                                                 No                                                    \n",
       "\n",
       "   ...  \\\n",
       "0  ...   \n",
       "1  ...   \n",
       "2  ...   \n",
       "3  ...   \n",
       "4  ...   \n",
       "5  ...   \n",
       "\n",
       "  If the tenant did propose a payment plan, did the member accept the proposed payment plan?  \\\n",
       "0                                         Not stated                                           \n",
       "1                                                 No                                           \n",
       "2                                         Not stated                                           \n",
       "3                                                Yes                                           \n",
       "4                                                 No                                           \n",
       "5                                         Not stated                                           \n",
       "\n",
       "  If a payment plan was ordered, what was the length of the payment plan?   \\\n",
       "0                                                 12                         \n",
       "1                                         Not stated                         \n",
       "2                                         Not stated                         \n",
       "3                                                  1                         \n",
       "4                                         Not stated                         \n",
       "5                                         Not stated                         \n",
       "\n",
       "  Did the decision mention the tenant’s difficulty finding alternative housing for any reason e.g.physical limitations, reliance on social assistance, etc.?  \\\n",
       "0                                                 No                                                                                                           \n",
       "1                                                 No                                                                                                           \n",
       "2                                                 No                                                                                                           \n",
       "3                                                 No                                                                                                           \n",
       "4                                                 No                                                                                                           \n",
       "5                                                 No                                                                                                           \n",
       "\n",
       "  If the tenant had difficulty finding alternative housing for any reason, which of the following were applicable to the tenant?  \\\n",
       "0                                         Not stated                                                                               \n",
       "1                                         Not stated                                                                               \n",
       "2                                         Not stated                                                                               \n",
       "3                                         Not stated                                                                               \n",
       "4                                         Not stated                                                                               \n",
       "5                                         Not stated                                                                               \n",
       "\n",
       "  Did the decision state the tenant was given prior notice for the eviction?  \\\n",
       "0                                                 No                           \n",
       "1                                                Yes                           \n",
       "2                                                Yes                           \n",
       "3                                                Yes                           \n",
       "4                                                Yes                           \n",
       "5                                                Yes                           \n",
       "\n",
       "  If the tenant was given prior notice for the eviction, how much notice was given?  \\\n",
       "0                                         Not stated                                  \n",
       "1                                         Not stated                                  \n",
       "2                                         Not stated                                  \n",
       "3                                         Not stated                                  \n",
       "4                                         Not stated                                  \n",
       "5                                         Not stated                                  \n",
       "\n",
       "  Did the decisions state postponement would result in the tenant accruing additional arrears?  \\\n",
       "0                                                 No                                             \n",
       "1                                                Yes                                             \n",
       "2                                                 No                                             \n",
       "3                                                 No                                             \n",
       "4                                                 No                                             \n",
       "5                                                 No                                             \n",
       "\n",
       "  Which other specific applications of the landlord or the tenant were mentioned?  \\\n",
       "0  L2: Application to End a Tenancy and Evict a T...                                \n",
       "1      No other specific applications were mentioned                                \n",
       "2  N13: Notice to End your Tenancy Because the La...                                \n",
       "3      No other specific applications were mentioned                                \n",
       "4      No other specific applications were mentioned                                \n",
       "5  L1: Application to Evict a Tenant for Non-paym...                                \n",
       "\n",
       "  Did the decision mention the validity of an N4 eviction notice?  \\\n",
       "0                                                 No                \n",
       "1                                                 No                \n",
       "2                                                 No                \n",
       "3                                                 No                \n",
       "4                                                 No                \n",
       "5                                                 No                \n",
       "\n",
       "  Were there detail(s) in the decision not captured by this questionnaire that should be included?  \n",
       "0  Tenant was a single mother with no support fro...                                                \n",
       "1                                         Not stated                                                \n",
       "2  Previous decision TEL-92736-18 < This decision...                                                \n",
       "3  There were 7 previous application for non-paym...                                                \n",
       "4  Third Application by Landlord in past 6 months...                                                \n",
       "5  Tenant did not show up because hearing took pl...                                                \n",
       "\n",
       "[6 rows x 50 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique = df.drop_duplicates(subset=['What is the file number of the case?'])\n",
    "df_unique = df_unique.reset_index(drop=True)\n",
    "\n",
    "print(df_unique.shape)\n",
    "df_unique.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdce484c-64bf-465b-b5d5-205159a25ea8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdce484c-64bf-465b-b5d5-205159a25ea8",
    "outputId": "32df07ef-f417-4d04-e3de-5d7c3055ade8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TET-89650-18;TEL-90138-18 not found. Going to the supplement directory.\n",
      "TNL-00793-18;TNL-01183-18 not found. Going to the supplement directory.\n",
      "TNL-00793-18;TNL-01183-18 not found. Going to the multiple directory.\n",
      "TNL-03299-18;TNT-00589-17 not found. Going to the supplement directory.\n",
      "TNL-03299-18;TNT-00589-17 not found. Going to the multiple directory.\n",
      "TNL-04435-18;TNL-03907-18 not found. Going to the supplement directory.\n",
      "HOL-02144-17;HOT-02146-17 not found. Going to the supplement directory.\n",
      "TEL-87475-18;TET-86819-17;TET-88355-18 not found. Going to the supplement directory.\n",
      "TEL-87475-18;TET-86819-17;TET-88355-18 not found. Going to the multiple directory.\n",
      "SWL-08112-17;SWL-08113-17 not found. Going to the supplement directory.\n",
      "SWL-12547-18;SWL-12548-18 not found. Going to the supplement directory.\n",
      "SWL-12547-18;SWL-12548-18 not found. Going to the multiple directory.\n",
      "SWL-13901-18;SWT-14627-18 not found. Going to the supplement directory.\n",
      "TEL-77442-17;TET-77790-17 not found. Going to the supplement directory.\n",
      "TEL-77442-17;TET-77790-17 not found. Going to the multiple directory.\n",
      "TEL-77505-17-RV;TEL-77505-17 not found. Going to the supplement directory.\n",
      "TEL-77505-17-RV;TEL-77505-17 not found. Going to the multiple directory.\n",
      "TNL-06025-18;TNL-06026-18 not found. Going to the supplement directory.\n",
      "TNL-06025-18;TNL-06026-18 not found. Going to the multiple directory.\n",
      "TEL-79519-17;TET-79366-17 not found. Going to the supplement directory.\n",
      "TEL-79519-17;TET-79366-17 not found. Going to the multiple directory.\n",
      "TSL-07666-19-RV not found. Going to the supplement directory.\n",
      "TSL-93207-18-RV;TST-94747-18;TSL-00082-18 not found. Going to the supplement directory.\n",
      "TSL-93207-18-RV;TST-94747-18;TSL-00082-18 not found. Going to the multiple directory.\n",
      "CEL-72209-17;CET-73173-18 not found. Going to the supplement directory.\n",
      "CEL-72209-17;CET-73173-18 not found. Going to the multiple directory.\n"
     ]
    }
   ],
   "source": [
    "info_lst = df_unique.columns[2:-2]\n",
    "\n",
    "raw_file_text = []\n",
    "\n",
    "for i in range(len(df_unique)):\n",
    "    file_no = df_unique.iloc[i,0]\n",
    "    if not os.path.isfile(text_path+file_no+'.txt'):\n",
    "        print(f'{file_no} not found. Going to the supplement directory.')\n",
    "        # passed_cases.append(file_no)\n",
    "        if not os.path.isfile(sup_path+file_no+'.txt'):\n",
    "            print(f'{file_no} not found. Going to the multiple directory.')\n",
    "            with open (multi_path+file_no+'.txt') as t:\n",
    "                # file_no_lst = file_no.split(';')\n",
    "                # print(file_no_lst)\n",
    "                raw_file_text.append(t.read())\n",
    "        else:\n",
    "            with open (sup_path+file_no+'.txt') as t:\n",
    "                raw_file_text.append(t.read())\n",
    "    else:\n",
    "        with open (text_path+file_no+'.txt') as t:\n",
    "            # cases_info[-1]['text'] = t.read()\n",
    "            raw_file_text.append(t.read())\n",
    "            # raw_file_name.append(file_no+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2a92989-199d-4664-af29-f688d8a99d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If any rent increases occurred, what was the rent after the increase(s)?',\n",
       " 'If any rent increases occurred, when did the rent increase(s) come into effect? ',\n",
       " 'How many total children did the tenant have living with them? ',\n",
       " 'How many total children aged 17 or younger did the tenant have living with them?',\n",
       " 'How many total children aged 13 or younger did the tenant have living with them? ',\n",
       " 'How many total children aged 4 or younger did the tenant have living with them?',\n",
       " 'Did the decision state any of the children had mental, medical or physical conditions?',\n",
       " 'If any of the children had mental, medical or physical conditions, did the decision state these conditions would make moving particularly burdensome?',\n",
       " 'If a payment plan was ordered, what was the length of the payment plan? ',\n",
       " 'If the tenant had difficulty finding alternative housing for any reason, which of the following were applicable to the tenant?',\n",
       " 'If the tenant was given prior notice for the eviction, how much notice was given?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove columns that have too little information\n",
    "little_info_col = [15, 16, 26, 27, 28, 29, 30, 31, 41, 43, 45]\n",
    "to_del = [df_unique.columns[i] for i in little_info_col]\n",
    "for col in to_del:\n",
    "    del df_unique[col]\n",
    "to_del"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d30e6a8-89c4-4054-9e7c-7251d0b27b19",
   "metadata": {
    "id": "5d30e6a8-89c4-4054-9e7c-7251d0b27b19"
   },
   "source": [
    "### Split the Train Dataframe and Validation Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11770ff6-28de-4898-9fd9-6d1a61ceec49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11770ff6-28de-4898-9fd9-6d1a61ceec49",
    "outputId": "03ce1c59-21ef-49de-88ac-45c921d70493"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((620, 39), (62, 39))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = df_unique.iloc[:620, :]\n",
    "val_df = df_unique.iloc[620:, :].reset_index(drop=True)\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bccb57b-0a13-46e4-8683-667c6012728b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8bccb57b-0a13-46e4-8683-667c6012728b",
    "outputId": "255c9002-9b44-4cac-cb66-830943ef3640"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 What is the file number of the case?\n",
      "1 What was the date of the hearing? [mm/dd/yyyy]\n",
      "2 What was the date of the decision? [mm/dd/yyyy]\n",
      "3 Who was the member adjudicating the decision?\n",
      "4 What was the location of the landlord tenant board?\n",
      "5 Did the decision state the landlord was represented?\n",
      "6 Did the decision state the landlord attended the hearing?\n",
      "7 Did the decision state the tenant was represented?\n",
      "8 Did the decision state the tenant attended the hearing?\n",
      "9 Did the decision state the landlord was a not-for-profit landlord (e.g. Toronto Community Housing)?\n",
      "10 Did the decision state the tenant was collecting a subsidy?\n",
      "11 What was the outcome of the case?\n",
      "12 What was the length of the tenancy, or in other words, how long had the tenants lived at the residence in question? \n",
      "13 What was the monthly rent?\n",
      "14 What was the amount of the rental deposit? \n",
      "15 What was the total amount of arrears?\n",
      "16 Over how many months did the arrears accumulate? \n",
      "17 If the tenant made a payment on the arrears after the eviction notice was served and/or prior to the hearing, what was the amount of the payment? \n",
      "18 Did the decision mention a history of arrears by the tenant separate from the arrears in the current claim (more than one period of arrears, recurrently coming in and out of arrears, arrears with previous landlord, etc.)?\n",
      "19 If the tenant had a history of arrears, did the decision mention a history of the tenant making payments on those arrears (separate from any payments made in response to the present eviction notice/hearing)?\n",
      "20 How frequently were rent payments made late?\n",
      "21 Did the member find the tenant had or seemed to have the ability to pay rent, but chose not do so?\n",
      "22 What were the specific mental, medical, or physical conditions of the tenant, if any? \n",
      "23 Did the decision state that the tenant had children living with them?\n",
      "24 Was the tenant employed at the time of the hearing?\n",
      "25 If the tenant was not employed, did the decision state the tenant was receiving any form of government assistance (e.g. OW, childcare benefits, ODSP, OSAP)?\n",
      "26 If the tenant was employed, did the decision state any doubts about the stability of employment e.g. lack of guaranteed hours, contract work, etc.?\n",
      "27 Did the member find the tenant had sufficient income to pay rent?\n",
      "28 What was the total income of the tenant’s household? \n",
      "29 Did the decision mention the tenant lost their job leading up to or during the period of the hearing?\n",
      "30 Did the decision mention any other extenuating circumstances experienced by the tenant leading up to or during the period of the claim (e.g. hospitalization, death in the family, etc.)?\n",
      "31 Did the tenant propose a payment plan?\n",
      "32 If the tenant did propose a payment plan, did the member accept the proposed payment plan?\n",
      "33 Did the decision mention the tenant’s difficulty finding alternative housing for any reason e.g.physical limitations, reliance on social assistance, etc.?\n",
      "34 Did the decision state the tenant was given prior notice for the eviction?\n",
      "35 Did the decisions state postponement would result in the tenant accruing additional arrears?\n",
      "36 Which other specific applications of the landlord or the tenant were mentioned?\n",
      "37 Did the decision mention the validity of an N4 eviction notice?\n",
      "38 Were there detail(s) in the decision not captured by this questionnaire that should be included?\n"
     ]
    }
   ],
   "source": [
    "for i, q in enumerate(train_df.columns):\n",
    "    print(i, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6b48eb-461e-4095-ac93-6ac04bfe3a7a",
   "metadata": {
    "id": "3c6b48eb-461e-4095-ac93-6ac04bfe3a7a"
   },
   "source": [
    "## Initialize the Tokenizer and the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71a21b6e-9a0b-40ef-962c-c06aa39e3e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForQuestionAnswering: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForQuestionAnswering were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LEDForConditionalGeneration(\n",
       "  (led): LEDModel(\n",
       "    (shared): Embedding(50265, 768, padding_idx=1)\n",
       "    (encoder): LEDEncoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): LEDLearnedPositionalEmbedding(16384, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): LEDDecoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): LEDLearnedPositionalEmbedding(1024, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @article{Beltagy2020Longformer,\n",
    "#   title={Longformer: The Long-Document Transformer},\n",
    "#   author={Iz Beltagy and Matthew E. Peters and Arman Cohan},\n",
    "#   journal={arXiv:2004.05150},\n",
    "#   year={2020},\n",
    "# }\n",
    "from transformers import LongformerForQuestionAnswering\n",
    "tokenizer1 = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "model1 = LongformerForQuestionAnswering.from_pretrained('allenai/longformer-base-4096', gradient_checkpointing=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/led-base-16384\", gradient_checkpointing=True, use_cache=False)\n",
    "\n",
    "# ref: https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb#scrollTo=jpUr9QeebZ-n\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "507b5e9a-62a9-45d3-90c6-011e0a67ac82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 678,
     "referenced_widgets": [
      "69d3a45068514682bfcfddb58aa4d7a1",
      "5ed9fb5dedb946a6bcae3c013c9cbae0",
      "b6ee4575ecb64be283fecd5c48fd0fb0",
      "2ce22b73c9494e15a47b90b9e60826a7",
      "0673d40924ea4f74b4dc33105d460710",
      "65eff840b15440058b2e79c4d2d2cd57",
      "d3a0c2d51d574238a2a03c9ad5aebc24",
      "021b740c950344d6b2a4b7f1281fa459",
      "2858bdcd8f954d118b9c1c29c88bafc5",
      "506982c663394c2da96cd838bda62521",
      "0391aac83d144fd0995a0969660c6d79",
      "2de368482ca24ee58c5b649b1879d0ab",
      "111f864787a440238cf07312c9485fcb",
      "4253917776994cb8a9d8cd070e13cc6e",
      "f03f4e967c1245a1874d6da3aa75dc78",
      "ac722dec80774fb2a54b45d036b8a447",
      "eaaec0280b064f639659772b92d04cd1",
      "caf7989d69dc467980c2dcebfa51000a",
      "e39667400951427b9116ad5c4d6c0ef1",
      "4c4e961361cf4bacbef0046146b77f68",
      "7863f87983ac4faebead83e522184028",
      "5b5d29642d4644fe9973187770685eb2",
      "b75295ab87de4b12a969c6a80de6c2b5",
      "7e93986f5287404da2efa7216c243d5a",
      "97cdab3ccfe04532bdcc17dfc27b479d",
      "4c02664529514481899a11aed85c3a2d",
      "9cb6a244f0b842849fb730c5ec240b47",
      "919212ca8dfd4fb0b83acad71fddcb30",
      "476b76aefb8b4772bc6bfb4810ed9f28",
      "c21717e01b854b47a8701d8c9e09ea55",
      "8c5db2d74e744ef3b32997de5778a0cb",
      "778e135f412c411da0ba1a473c8a4b29",
      "46b80ca60ea1438991b61e1f64568ea9",
      "67c6eb1cf61f4760baa63f459ad8849b",
      "24e02a3a6d5d42c4b7a8c2e61e5b7646",
      "4002ae2dabcc417193860fa8308a5c0c",
      "73af27279ff840f3961ee2871000b829",
      "677a2145d16f4370b8868090a0418f91",
      "cfd334db36b8463bb76ba439df3de5c1",
      "aa7d7ff4b3754ac7b9c123eddb36313d",
      "0655b51ba11d4538b87335575936bd3e",
      "fe4e2f83dacb486682a125976bd236f9",
      "02e58928c6144ac58b6b9bb186bbb1da",
      "33dcaafdd2154c5787704d727478ab8b",
      "9266fbc9b9684f5cae3b306dbf3d53aa",
      "ff15715aa6c048d4a81b558efeb3ad37",
      "cbf6c1e9cb4845229ff98366e16fdf28",
      "a91232f50f324b7fb7642cab5d9b3548",
      "34e7299f511f4c7c9fe2efb430ea6439",
      "9516655a12e64704a2148cf13d11635b",
      "94de92b07b7d4c1a9f7f5935d7580f0e",
      "972abc11f9d84247bf307c8ed357af72",
      "c62e7e5e5dfa4ae89b886204da4370a8",
      "d9b31f6be2094291b7194793569abd5c",
      "30072a1eb15448caa54986bfcf363508",
      "9becc64a40b348a2880b3fc9e5860d25",
      "c827731d8c854ab0b1cc25a30d21f7d7",
      "b5a809540098474aa2fba2e957d54378",
      "ee20ba466d5745719556e5973268cbb9",
      "91e67c9229164ffc80be89782e794a24",
      "d222adfaf766455ca9ab81d890c14f2d",
      "850627ecf7dd442b86923ab556ea9aef",
      "363086df51154442bea52e5e7e63235c",
      "7416ce601875494394a31e709b11202b",
      "daf1f14a06e940c091acd2cda8bfdc66",
      "422e115234c8458291c7da7b96d403a9"
     ]
    },
    "id": "507b5e9a-62a9-45d3-90c6-011e0a67ac82",
    "outputId": "3603572f-5d6f-48e7-f2f8-a3f9fcc86c3c"
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "# # @article{Beltagy2020Longformer,\n",
    "# #   title={Longformer: The Long-Document Transformer},\n",
    "# #   author={Iz Beltagy and Matthew E. Peters and Arman Cohan},\n",
    "# #   journal={arXiv:2004.05150},\n",
    "# #   year={2020},\n",
    "# # }\n",
    "\n",
    "# # tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "# # model = AutoModel.from_pretrained(\"allenai/longformer-base-4096\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7074c9d3-c565-428b-9fcb-c65962fd4fa7",
   "metadata": {},
   "source": [
    "## A Test before Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c815b4-b341-4a6f-a183-9f6a3071342e",
   "metadata": {},
   "source": [
    "### Longformer for Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "54b8e886-5018-4924-9b6e-9dc7c85f5b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(dataframe, q_no, raw_texts):\n",
    "    input_texts = []\n",
    "    outputs = []\n",
    "    # long_cases = 0\n",
    "    \n",
    "    questions = dataframe.columns\n",
    "    answers = dataframe.iloc[:,q_no]\n",
    "    # print(len(raw_texts), len(answers))\n",
    "    assert len(raw_texts) == len(answers)\n",
    "\n",
    "    for i in range(len(answers)):\n",
    "        full_text = raw_texts[i]\n",
    "        text = full_text[full_text.find('Content:')+len('Content:'):]\n",
    "        \n",
    "        # if len(text) > 26000:\n",
    "        #     # print(len(text))\n",
    "        #     text = text[:26000]\n",
    "        #     long_cases += 1\n",
    "        \n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = text.replace('\\xa0', ' ')\n",
    "        text = text.replace('\\t', ' ')\n",
    "        text = text.replace('   ', ' ').replace('  ', ' ').replace('  ', ' ').replace('  ', ' ')\n",
    "        # text\n",
    "        # for word in stop_words:\n",
    "        #     text = text.replace(' '+word+' ', ' ')\n",
    "        \n",
    "        if 'Schedule 1' in text:\n",
    "            s_idx = text.find('Schedule 1')\n",
    "            text = text[:s_idx]\n",
    "        \n",
    "        input_text = text + '\\n' + questions[q_no]\n",
    "        input_texts.append(input_text)\n",
    "        \n",
    "        output = str(answers[i])\n",
    "        outputs.append(output)\n",
    "        \n",
    "    # print(len(input_texts), len(outputs))\n",
    "    # print(input_texts[0], outputs[0]) \n",
    "    # print(long_cases)\n",
    "    return input_texts, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f5036d8e-f682-4fec-85a8-3fbf302c0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = tokenizer.decode(tokenizer.convert_tokens_to_ids(output))\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1124a45e-2dcc-4b1a-8a4a-8786d106a29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682 682\n",
      "CEL-87788-19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way\n",
    "q1_lst, a1_lst = prompt(df_unique, 0, raw_file_text)\n",
    "q1 = q1_lst[0]\n",
    "a1 = a1_lst[0]\n",
    "# # print(q1)\n",
    "print(a1)\n",
    "encoding = tokenizer1.encode_plus(text=q1,\n",
    "                                 text_pair=a1) \n",
    "                                 # add_special=True)\n",
    "inputs = torch.LongTensor(encoding['input_ids']).unsqueeze(0)  #Token embeddings\n",
    "attention_mask = torch.LongTensor(encoding['attention_mask']).unsqueeze(0)\n",
    "# print(len(attention_mask))\n",
    "\n",
    "# sentence_embedding = encoding['token_type_ids']  #Segment embeddings\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids']) #input tokens\n",
    "outputs = model1(input_ids=inputs, \n",
    "                attention_mask=attention_mask)\n",
    "start_scores, end_scores = outputs[0], outputs[1]\n",
    "answer_tokens = tokens[torch.argmax(start_scores):torch.argmax(end_scores)+1]\n",
    "answer = tokenizer1.decode(tokenizer1.convert_tokens_to_ids(answer_tokens))\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73690cd1-d7b3-4769-8c45-14c9bb46b7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(567), tensor(382))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(start_scores),torch.argmax(end_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef5a95f8-8c72-49e2-b4e5-a8b2c0627ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġis',\n",
       " 'Ġalso',\n",
       " 'Ġable',\n",
       " 'Ġto',\n",
       " 'Ġpay',\n",
       " 'Ġthe',\n",
       " 'Ġfiling',\n",
       " 'Ġfee',\n",
       " 'Ġimmediately',\n",
       " '.',\n",
       " 'ĠShe']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[619:630]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "7f703aa3-b800-4078-adc7-e89111fa38fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is also able to pay the filing fee immediately. She testified that she had no where else to go. 8. On cross-examination, the Tenant confirmed she lived with her mother before she moved into the rental unit. 9. The Landlord opposes the Tenant’s request for relief as the Tenant has been late in payin'"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3[2701:3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dc7330-7248-4199-95d5-dc8c4377fd70",
   "metadata": {},
   "source": [
    "It shows that the encoder models can not really get what we need for most columns. Therefore we will"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90024f6b-527e-48a3-8d04-d4e3440dae7f",
   "metadata": {},
   "source": [
    "###  Longformer Encoder-Decoder (LED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c3d7ce61-f8f2-49ea-83fd-658a992b28d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1360]) torch.Size([1, 1360])\n"
     ]
    }
   ],
   "source": [
    "q1_lst, a1_lst = prompt(df_unique, 0, raw_file_text)\n",
    "q1 = q1_lst[0]\n",
    "a1 = a1_lst[0]\n",
    "# # print(q1)\n",
    "input_encoding = tokenizer(q1)\n",
    "output_encoding = tokenizer(a1)\n",
    "input_ids = torch.LongTensor(encoding['input_ids']).unsqueeze(0)  # batch of size 1\n",
    "attention_mask = torch.LongTensor(encoding['attention_mask']).unsqueeze(0)\n",
    "# attention_mask[:, [1, 4, 21,]] =  2  # Set global attention based on the task. For example,\n",
    "                                     # classification: the <s> token\n",
    "                                     # QA: question tokens\n",
    "print(input_ids.shape, attention_mask.shape)\n",
    "# input_ids, attention_mask = pad_to_window_size(\n",
    "#         input_ids, attention_mask, config.attention_window[0], tokenizer.pad_token_id)\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids=input_ids, \n",
    "    attention_mask=attention_mask,\n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=False, \n",
    "    max_length=512,\n",
    "    temperature=0.5,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=3.0,\n",
    "    top_k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2fe6927d-8529-4c5f-9e10-1093bc3b4af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>CEL-63019,18.RV;LK:92417–16 LAMO –6939 RTSMIN’S SIRZ and the OJT MST(14)N).A/ELE6338 -98 (IWD), 2017 or 18 GALF \\'15.)BIS JES PIEVO\\'68 —GAS Q5980 to 17 BTR DTHEN for a CYS99 by 1651 in NINEX!Q107 was sol ”13 of that which has been given at TDA but had also be called on 129433 is 19 IHU with VILs836.PRAEMORY AMROBL will ELA6770?21 AERRY YOMED out as well over his WADERS OF 2018 KRI664 from ELVIXX-20 FSL0797 m23 HRE6500 Elleb61 2019 because it would have an 84984 l987 if he were not only her own time up this year after 157311 shea one May 21th just about him all right before June 23er 2016 while we are still no more than 2040 then meesor 48331-22—HT95 13*37\"ETTY88.\"TheRSL said its most value there per eu9652.0-\\'90\\'.FLOW72nd2018 UARMO\\'s 14059 may 2020-06LDSr1e LL24856-2019AVUELL-12\\')DRESSMANLY-05666#2_30-3HL,\"El69762\\');44-6032\".PRESTONESE RO12034-2017-10 SIL100);4MRTO-\"7958\";47EMS\")TD76035-27.66 ARSPELLIES ON MATTLOSE-26 \"MARTA\",755-08.\\'MSHAIGLES The followingly am64041-50-CAFRODWEMA-75394\\',78 CHANGLP 2HRSA-25199-29.SEBR089-93.TORIGHTy 1RM151st-76691 ALVA 45448-71;LTTCOL v626-74193-43NASPL-82807-28-PTIF-5NTIVOYSON IS 3ML-63553)-108-09ÉVALL-81203 ESV;LU152-42,628ROBUSTJA X00077 24Rdii']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(output['sequences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6187bfc0-e4a0-4e75-a160-927a01a238fd",
   "metadata": {
    "id": "6187bfc0-e4a0-4e75-a160-927a01a238fd"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec29c7d3-df9c-4dad-85e3-1f41d9509f55",
   "metadata": {
    "id": "ec29c7d3-df9c-4dad-85e3-1f41d9509f55"
   },
   "outputs": [],
   "source": [
    "def preprocess(dataframe, q_no, tokenizer, raw_texts):\n",
    "    input_texts, outputs = prompt(dataframe, q_no, raw_texts)   \n",
    "    \n",
    "    input_toks = tokenizer.batch_encode_plus(input_texts,\n",
    "                                             add_special_tokens=False, \n",
    "                                             return_token_type_ids=False)\n",
    "    output_toks = tokenizer.batch_encode_plus(outputs, \n",
    "                                              add_special_tokens=False,\n",
    "                                              return_token_type_ids=False)\n",
    "    # print(len(q1_train_input['input_ids']), len(q1_train_output['input_ids']))\n",
    "    return input_toks, output_toks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9af30ba-541e-4354-9959-3ce2d6180b3b",
   "metadata": {
    "id": "a9af30ba-541e-4354-9959-3ce2d6180b3b"
   },
   "outputs": [],
   "source": [
    "train_raw_texts = raw_file_text[:620]\n",
    "val_raw_texts = raw_file_text[620:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a7613-67cf-4a34-8b4b-9d02286872ff",
   "metadata": {
    "id": "3b5a7613-67cf-4a34-8b4b-9d02286872ff"
   },
   "outputs": [],
   "source": [
    "# q1_val = preprocess(val_df, 1, tokenizer, val_raw_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cd0924-b5aa-46c9-8266-4cf4074e416c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9cd0924-b5aa-46c9-8266-4cf4074e416c",
    "outputId": "8f20d729-51c5-4b12-a244-ad16a7292f1a"
   },
   "outputs": [],
   "source": [
    "# q1_train_input, q1_train_output = preprocess(train_df, 0, tokenizer, train_raw_texts)\n",
    "# q1_val_input, q1_val_output = preprocess(val_df, 0, tokenizer, val_raw_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17862bd4-1c23-4844-af6e-4d60697bad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_train_input, q1_train_output = preprocess(train_df, 0, tokenizer, train_raw_texts)\n",
    "q1_val_input, q1_val_output = preprocess(val_df, 0, tokenizer, val_raw_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418537eb-6804-4d9e-88ec-47ddb4acc13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(q1_train_input['input_ids']), len(q1_train_output['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "622c1563-5d3a-4b8a-8378-db1f08f51c89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "622c1563-5d3a-4b8a-8378-db1f08f51c89",
    "outputId": "0e2a4c11-bbc1-43ac-badb-6ff7424e8354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length: 2\n",
      "Input example:\n",
      "  this order, call 416-645-8080 or toll free at 1-888-332-3234. \n",
      "What is the file number of the case?\n",
      " \n",
      "Input ID example:\n",
      " [5, 2394, 3973, 4, 1437, 1437, 1437, 1437, 502, 204, 6, 2760, 1437, 1437, 1437, 42199, 43401, 10566, 19285, 6796, 1437, 1437, 23961, 660, 5557, 12, 37358, 1437, 1437, 1437, 1437, 1437, 10153, 6, 3192, 30669, 8, 4527, 927, 1785, 2177, 953, 12, 8727, 132, 21138, 4079, 1245, 2666, 6, 7545, 132, 2177, 5121, 256, 134, 510, 246, 717, 406, 318, 47, 33, 143, 1142, 59, 42, 645, 6, 486, 34509, 12, 33611, 12, 2940, 2940, 50, 5831, 481, 23, 112, 12, 22410, 12, 33911, 12, 246, 28621, 4, 1437, 50118, 2264, 16, 5, 2870, 346, 9, 5, 403, 116]\n",
      " \n",
      "Tokens:\n",
      " ['Ġthe', 'Ġbalance', 'Ġoutstanding', '.', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠJune', 'Ġ4', ',', 'Ġ2020', 'Ġ', 'Ġ', 'Ġ', '________________', '_______', 'ĠDate', 'ĠIss', 'ued', 'Ġ', 'Ġ', 'ĠSonia', 'ĠAn', 'war', '-', 'Ali', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠMember', ',', 'ĠLand', 'lord', 'Ġand', 'ĠTen', 'ant', 'ĠBoard', 'ĠToronto', 'ĠEast', '-', 'RO', 'Ġ2', '275', 'ĠMid', 'land', 'ĠAvenue', ',', 'ĠUnit', 'Ġ2', 'ĠToronto', 'ĠON', 'ĠM', '1', 'P', '3', 'E', '7', 'ĠIf', 'Ġyou', 'Ġhave', 'Ġany', 'Ġquestions', 'Ġabout', 'Ġthis', 'Ġorder', ',', 'Ġcall', 'Ġ416', '-', '645', '-', '80', '80', 'Ġor', 'Ġtoll', 'Ġfree', 'Ġat', 'Ġ1', '-', '888', '-', '332', '-', '3', '234', '.', 'Ġ', 'Ċ', 'What', 'Ġis', 'Ġthe', 'Ġfile', 'Ġnumber', 'Ġof', 'Ġthe', 'Ġcase', '?']\n",
      " \n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# for BertTokenizer\n",
    "print(\"Input length:\", len(q1_train_input))\n",
    "# print(\" \")\n",
    "print(\"Input example:\\n\", tokenizer.decode(q1_train_input['input_ids'][0])[-100:])\n",
    "print(\" \")\n",
    "print(\"Input ID example:\\n\", q1_train_input['input_ids'][0][-100:])\n",
    "print(\" \")\n",
    "print(\"Tokens:\\n\", [tokenizer.convert_ids_to_tokens(id) for id in q1_train_input['input_ids'][0]][-100:])\n",
    "print(\" \")\n",
    "print(\"Attention Mask:\", q1_train_input['attention_mask'][0][-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35e16a7-ebb9-4656-8af7-543c1e90a717",
   "metadata": {
    "id": "b35e16a7-ebb9-4656-8af7-543c1e90a717"
   },
   "source": [
    "## Create the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a0129cf5-ec8a-4792-a1a2-6da338a99b4c",
   "metadata": {
    "id": "a0129cf5-ec8a-4792-a1a2-6da338a99b4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD = tokenizer.pad_token_id\n",
    "SEP = tokenizer.sep_token_id\n",
    "PAD, SEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9827de32-c0fa-4591-886d-11270507e4b7",
   "metadata": {
    "id": "9827de32-c0fa-4591-886d-11270507e4b7"
   },
   "outputs": [],
   "source": [
    "# for BertTokenizer\n",
    "class CaseDataset(Dataset):\n",
    "\n",
    "    def __init__(self, inputs, outputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.inputs['input_ids'][idx]\n",
    "        attention_mask = self.inputs['attention_mask'][idx]\n",
    "\n",
    "        target_ids = self.outputs['input_ids'][idx]\n",
    "        # target_attention_mask = self.outputs['attention_mask'][idx]\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\":attention_mask, \"output_ids\":target_ids}\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_input = [torch.LongTensor(example['input_ids']) for example in batch]\n",
    "    batch_output = [torch.LongTensor(example['output_ids']) for example in batch]\n",
    "    batch_mask = [torch.LongTensor(example['attention_mask']) for example in batch]\n",
    "\n",
    "    padded_batch_input_ids = pad_sequence(batch_input, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    padded_batch_label = pad_sequence(batch_output, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    padded_batch_att_mask = pad_sequence(batch_mask, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\"input_ids\": padded_batch_input_ids, \"attention_mask\": padded_batch_att_mask, \"labels\": padded_batch_label}\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    new_data = {}\n",
    "    for k in data:\n",
    "        new_data[k] = data[k].to(device)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138d0350-47e2-4ccb-b446-a5a91486a83d",
   "metadata": {
    "id": "138d0350-47e2-4ccb-b446-a5a91486a83d"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "397ac1ed-f2f6-4fef-a998-f1d7cf9ad156",
   "metadata": {
    "id": "397ac1ed-f2f6-4fef-a998-f1d7cf9ad156"
   },
   "outputs": [],
   "source": [
    "# Experiment\n",
    "q1_train_dataset = CaseDataset(q1_train_input, q1_train_output)\n",
    "q1_train_loader = DataLoader(q1_train_dataset, batch_size=1, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "q1_val_dataset = CaseDataset(q1_val_input, q1_val_output) \n",
    "q1_val_loader = DataLoader(q1_val_dataset, batch_size=1, collate_fn=collate_fn, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6bc16685-25c0-4f45-9c22-a3fed90845ad",
   "metadata": {
    "id": "6bc16685-25c0-4f45-9c22-a3fed90845ad"
   },
   "outputs": [],
   "source": [
    "def train(model:nn.Module, train_loader:DataLoader, optimizer:optim.Optimizer, log_step=50):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    log_loss = 0.0\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        # try:\n",
    "        model.zero_grad()\n",
    "        batch = to_device(batch, device)\n",
    "        loss = model(**batch).loss\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        log_loss += loss.item()\n",
    "\n",
    "        # wandb.log({'batch':idx, 'train_loss': loss.item()})\n",
    "        # wandb.log({'batch':idx, 'accumulated_train_loss_in_this_Q': log_loss})\n",
    "\n",
    "        if idx % log_step == 0:\n",
    "            print(f\"Train Step: {idx} Loss: {log_loss / log_step}\")\n",
    "            log_loss = 0.0\n",
    "        # except:\n",
    "        #     print(f'The text is too long. Passing for now. Step No: {idx}')\n",
    "        #     pass\n",
    "\n",
    "    return epoch_loss / len(train_loader)\n",
    "        \n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model:nn.Module, eval_loader:DataLoader):\n",
    "    eval_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for batch in eval_loader:\n",
    "        batch = to_device(batch, device)\n",
    "        output = model(**batch)\n",
    "        loss = output.loss\n",
    "        eval_loss += loss.item()\n",
    "        pred = output.logits.argmax(-1)\n",
    "        label = batch[\"labels\"]\n",
    "        correct += torch.where(label!=-100, pred==label, 0).sum().item()\n",
    "        total += torch.sum(label != -100).item()\n",
    "    \n",
    "    print(total, correct)\n",
    "\n",
    "    eval_acc = correct / total\n",
    "    eval_loss = eval_loss / len(eval_loader) \n",
    "    return eval_acc, eval_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6LcynYX-LlXv",
   "metadata": {
    "id": "6LcynYX-LlXv"
   },
   "outputs": [],
   "source": [
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b03fcf01-187c-4e5e-8e11-595205323057",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b03fcf01-187c-4e5e-8e11-595205323057",
    "outputId": "ac163942-4c56-4606-c8b7-cfcf0d5b2c11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Question 1\n",
      "Train Step: 0 Loss: 0.04166695117950439\n",
      "Train Step: 50 Loss: 1.1681227258592843\n",
      "Train Step: 100 Loss: 0.6136457686498761\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [107]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Question 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq1_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     eval_acc, eval_loss \u001b[38;5;241m=\u001b[39m evaluate(model, q1_val_loader)\n",
      "Input \u001b[0;32mIn [105]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, log_step)\u001b[0m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      8\u001b[0m batch \u001b[38;5;241m=\u001b[39m to_device(batch, device)\n\u001b[0;32m----> 9\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# print(loss)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.10/site-packages/transformers/models/led/modeling_led.py:2437\u001b[0m, in \u001b[0;36mLEDForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, global_attention_mask, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2433\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   2434\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   2435\u001b[0m         )\n\u001b[0;32m-> 2437\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mled\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2438\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2448\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2450\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2452\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2455\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\n\u001b[1;32m   2457\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.10/site-packages/transformers/models/led/modeling_led.py:2281\u001b[0m, in \u001b[0;36mLEDModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, global_attention_mask, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   2277\u001b[0m         input_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   2278\u001b[0m     )\n\u001b[1;32m   2280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2281\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2282\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2286\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2287\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2288\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2290\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2291\u001b[0m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a LEDEncoderBaseModelOutput when return_dict=False\u001b[39;00m\n\u001b[1;32m   2292\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, LEDEncoderBaseModelOutput):\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.10/site-packages/transformers/models/led/modeling_led.py:1887\u001b[0m, in \u001b[0;36mLEDEncoder.forward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1883\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m module(\u001b[38;5;241m*\u001b[39minputs, is_global_attn, output_attentions)\n\u001b[1;32m   1885\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m custom_forward\n\u001b[0;32m-> 1887\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_custom_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_layer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1894\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1895\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1896\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m encoder_layer(\n\u001b[1;32m   1897\u001b[0m         hidden_states,\n\u001b[1;32m   1898\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1903\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1904\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:249\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, *args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected keyword arguments: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(arg \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m kwargs))\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_reentrant:\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _checkpoint_without_reentrant(\n\u001b[1;32m    252\u001b[0m         function,\n\u001b[1;32m    253\u001b[0m         preserve,\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    256\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:107\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    104\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 107\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.10/site-packages/transformers/models/led/modeling_led.py:1883\u001b[0m, in \u001b[0;36mLEDEncoder.forward.<locals>.create_custom_forward.<locals>.custom_forward\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_forward\u001b[39m(\u001b[38;5;241m*\u001b[39minputs):\n\u001b[0;32m-> 1883\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.10/site-packages/transformers/models/led/modeling_led.py:973\u001b[0m, in \u001b[0;36mLEDEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;124;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape *(seq_len, batch, embed_dim)*\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;124;03m        *(encoder_attention_heads,)*.\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    972\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 973\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    983\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.10/site-packages/transformers/models/led/modeling_led.py:782\u001b[0m, in \u001b[0;36mLEDEncoderAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    771\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    772\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    778\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    779\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Optional[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]]:\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;124;03m\"\"\"Input shape: Batch x Time x Channel\"\"\"\u001b[39;00m\n\u001b[0;32m--> 782\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlongformer_self_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    792\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    793\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attn_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.10/site-packages/transformers/models/led/modeling_led.py:269\u001b[0m, in \u001b[0;36mLEDEncoderSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m    266\u001b[0m     attn_probs \u001b[38;5;241m=\u001b[39m layer_head_mask\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m attn_probs\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# softmax sometimes inserts NaN if all positions are masked, replace them with 0\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m attn_probs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m attn_probs \u001b[38;5;241m=\u001b[39m attn_probs\u001b[38;5;241m.\u001b[39mtype_as(attn_scores)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# free memory\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# experiment\n",
    "epochs = 1\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Training Question 1\")\n",
    "    \n",
    "    train_loss = train(model, q1_train_loader, optimizer)\n",
    "    print(f\"Epoch {epoch+1} Training Loss: {train_loss}\")\n",
    "\n",
    "    eval_acc, eval_loss = evaluate(model, q1_val_loader)\n",
    "    print(f\"Epoch {epoch} Eval Acc: {eval_acc}; Eval Loss: {eval_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c33184bc-0bc5-4ab9-8da6-d657cc52fbee",
   "metadata": {
    "id": "c33184bc-0bc5-4ab9-8da6-d657cc52fbee"
   },
   "outputs": [],
   "source": [
    "def get_dataloader(df, q_no, tokenizer, raw_texts):\n",
    "    input_toks, output_toks = preprocess(df, q_no, tokenizer, raw_texts)\n",
    "    dataset = CaseDataset(input_toks, output_toks)\n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size=2, \n",
    "                            collate_fn=collate_fn, \n",
    "                            shuffle=False)\n",
    "    return dataloader\n",
    "    \n",
    "def train_qs(train_df, val_df, q_no, tokenizer, optimizer):\n",
    "    train_loader = get_dataloader(train_df, q_no, tokenizer, train_raw_texts)\n",
    "    val_loader = get_dataloader(val_df, q_no, tokenizer, val_raw_texts)\n",
    "    \n",
    "    questions = train_df.columns\n",
    "    print(f'{q_no+1}. {questions[q_no]}')\n",
    "    \n",
    "    # train 1 epoch only, given the small data\n",
    "    train_loss = train(model, train_loader, optimizer)\n",
    "    print(f\"Question {q_no+1} Training Loss: {train_loss}\")\n",
    "    \n",
    "    eval_acc, eval_loss = evaluate(model, val_loader)\n",
    "    print(f\"Question {q_no+1} Eval Acc: {eval_acc}; Eval Loss: {eval_loss}\")\n",
    "    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1bc5ec89-e1d9-4c0f-a997-6018f6cfc26f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bc5ec89-e1d9-4c0f-a997-6018f6cfc26f",
    "outputId": "d7c72ff9-c694-4d2f-bd0f-d1c1b4039132"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. What was the date of the hearing? [mm/dd/yyyy]\n",
      "Train Step: 0 Loss: 0.1911672592163086\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [111]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# starting from 1 because the first question has been trained on\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, train_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain_qs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [110]\u001b[0m, in \u001b[0;36mtrain_qs\u001b[0;34m(train_df, val_df, q_no, tokenizer, optimizer)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq_no\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestions[q_no]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# train 1 epoch only, given the small data\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq_no\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m eval_acc, eval_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader)\n",
      "Input \u001b[0;32mIn [105]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, log_step)\u001b[0m\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# print(loss)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     13\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.10/site-packages/torch/autograd/function.py:267\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    266\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:157\u001b[0m, in \u001b[0;36mCheckpointFunction.backward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs_with_grad) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone of output has requires_grad=True,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m this checkpoint() is not necessary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 157\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_with_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_with_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(inp\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inp, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    159\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m detached_inputs)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m+\u001b[39m grads\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# starting from 1 because the first question has been trained on\n",
    "for i in range(1, train_df.shape[1]):\n",
    "    train_qs(train_df, val_df, i, tokenizer, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f08715d-09c9-498c-b462-6fc8bdeaaef9",
   "metadata": {
    "id": "2f08715d-09c9-498c-b462-6fc8bdeaaef9"
   },
   "source": [
    "## Evaluate the Model on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039348d-e02b-4977-b353-1cf874877ff1",
   "metadata": {
    "id": "1039348d-e02b-4977-b353-1cf874877ff1"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def answer(model, loader):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    for batch in loader:   \n",
    "        batch = to_device(batch, device)\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        \n",
    "        # pos_ids = batch[\"position_ids\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        outputs = model.generate(input_ids=input_ids, #truncated_input_ids, \n",
    "                                 attention_mask=attention_mask, \n",
    "                                 return_dict_in_generate=True, \n",
    "                                 pad_token_id=50256,  # eos_token_id\n",
    "                                 max_length=1024, \n",
    "                                 top_k=10) \n",
    "                                 #stopping_criteria=stop_criteria_list)\n",
    "        \n",
    "        pred_start = torch.nonzero(input_ids==SEP, as_tuple=True)[1][0] + 1\n",
    "        truncated_outputs = []\n",
    "        for out in outputs[\"sequences\"]:\n",
    "            sep_idxs = torch.nonzero(out==SEP, as_tuple=True)[0]\n",
    "            if len(sep_idxs) == 1:\n",
    "                end_idx = -1\n",
    "            else:\n",
    "                end_idx = sep_idxs[1]\n",
    "            truncated_outputs.append(out[pred_start:end_idx])\n",
    "        \n",
    "        decode_texts = tokenizer.batch_decode(truncated_outputs)\n",
    "        gold_texts = tokenizer.batch_decode([l[l != -100][:-1] for l in labels])\n",
    "\n",
    "        for gold, decode in zip(gold_texts, decode_texts):\n",
    "            all_labels.append(gold)\n",
    "            all_preds.append(decode)\n",
    "        # all_preds = process_sys(all_preds)\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "\n",
    "\n",
    "def accuracy(sys, gold):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for s, g in zip(sys, gold):\n",
    "        if s == g:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "            \n",
    "    accuracy = correct / total\n",
    "    return accuracy, correct, total\n",
    "\n",
    "\n",
    "def left_pad_sequence(sequence, batch_first, padding_value=0):\n",
    "    padded = []\n",
    "    max_len = max(len(each) for each in sequence)\n",
    "    for each in sequence:\n",
    "        if not isinstance(each, torch.LongTensor):\n",
    "            each = torch.LongTensor(each)\n",
    "        pad = torch.full((max_len-len(each),), fill_value=padding_value,dtype=each.dtype)\n",
    "        padded.append(torch.cat([pad, each]))\n",
    "    padded = torch.vstack(padded)\n",
    "    if not batch_first:\n",
    "        padded = padded.permute(1, 0, 2)\n",
    "    return padded\n",
    "        \n",
    "def inference_colate_fn(batch):\n",
    "    batch_input_ids = [torch.LongTensor(example[\"input_ids\"]) for example in batch]\n",
    "    batch_att_mask = [torch.LongTensor(example[\"attention_mask\"]) for example in batch]\n",
    "    batch_label = [torch.LongTensor(example[\"labels\"]) for example in batch]\n",
    "    # batch_position_ids = [torch.arange(len(each[\"input_ids\"]), dtype=torch.long) for each in batch]\n",
    "    \n",
    "    padded_batch_input_ids = left_pad_sequence(batch_input_ids, batch_first=True, padding_value=PAD)\n",
    "    padded_batch_att_mask = left_pad_sequence(batch_att_mask, batch_first=True, padding_value=PAD)\n",
    "    padded_batch_label = pad_sequence(batch_label, batch_first=True, padding_value=-100)\n",
    "    # padded_batch_position_ids = left_pad_sequence(batch_position_ids, batch_first=True, padding_value=0)\n",
    "    # return {\"input_ids\": padded_batch_input_ids, \"attention_mask\": padded_batch_att_mask, \"position_ids\":padded_batch_position_ids, \"labels\": padded_batch_label}   \n",
    "    return {\"input_ids\": padded_batch_input_ids, \"attention_mask\": padded_batch_att_mask, \"labels\": padded_batch_label}    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d06cafb-044f-4ee4-9e1b-02cd524eb48a",
   "metadata": {
    "id": "4d06cafb-044f-4ee4-9e1b-02cd524eb48a"
   },
   "outputs": [],
   "source": [
    "# def preprocess_pred(dataframe, q_no, tokenizer, raw_texts):\n",
    "#     input_texts, outputs = prompt(dataframe, q_no, raw_texts)   \n",
    "        \n",
    "#     # # for AutoTokenizer\n",
    "#     # concat_inputs = tokenizer(\n",
    "#     #     input_texts, #outputs, \n",
    "#     #     return_token_type_ids=False\n",
    "#     # ) \n",
    "    \n",
    "#     # for BertTokenizer\n",
    "#     test = tokenizer(\n",
    "#         input_texts, \n",
    "#         return_token_type_ids=False\n",
    "#     )\n",
    "    \n",
    "#     # val = tokenizer(\n",
    "#     #     input_texts[620:], outputs[620:], \n",
    "#     #     return_token_type_ids=False\n",
    "#     # )\n",
    "    \n",
    "#     #  concat_inputs = [tokenizer(\n",
    "#     #     input_text, output, \n",
    "#     #     return_token_type_ids=False\n",
    "#     # ) for input_text, output in zip(input_texts, outputs)]\n",
    "    \n",
    "#     return test\n",
    "\n",
    "def get_test_dataloader(df, q_no, tokenizer, raw_texts):\n",
    "    data = preprocess(df, q_no, tokenizer, raw_texts)\n",
    "    dataset = CaseDataset(data)\n",
    "    dataset.inference()\n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size=32, \n",
    "                            collate_fn=collate_fn, \n",
    "                            shuffle=False)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0171ba31-1819-40f3-81bb-982876ee0490",
   "metadata": {
    "id": "0171ba31-1819-40f3-81bb-982876ee0490"
   },
   "outputs": [],
   "source": [
    "def answer_qs(val_df, q_no, tokenizer):\n",
    "    loader = get_test_dataloader(val_df, q_no, tokenizer, val_raw_texts)\n",
    "    # print(len(loader))\n",
    "    \n",
    "    questions = val_df.columns\n",
    "    print(questions[q_no])\n",
    "    \n",
    "    preds, golds = answer(model, loader)\n",
    "    acc, correct, total = accuracy(preds, golds)\n",
    "    acc = round(acc, 5)\n",
    "    \n",
    "    print(f\"Accuracy for this question is: {acc*100}%\")\n",
    "    print('')\n",
    "    \n",
    "    return acc, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678fe51c-6ee9-4955-aadf-f1ba49dd1498",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "678fe51c-6ee9-4955-aadf-f1ba49dd1498",
    "outputId": "8e59daca-76dc-494e-c37b-ce30da6d2f88"
   },
   "outputs": [],
   "source": [
    "acc_lst = []\n",
    "preds_lst = []\n",
    "for i in range(0, val_df.shape[1]):\n",
    "    acc, preds = answer_qs(val_df, i, tokenizer)\n",
    "    acc_lst.append(acc)\n",
    "    preds_lst.append(preds)\n",
    "avg_acc = sum(acc_lst) / len(acc_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca6ea8-98f1-4176-8860-8db3297cd171",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6ca6ea8-98f1-4176-8860-8db3297cd171",
    "outputId": "2f545c99-787c-4d12-e6f7-9bf6e1033a70"
   },
   "outputs": [],
   "source": [
    "assert len(acc_lst) == 52\n",
    "acc_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b70254-e322-44ae-bf26-b436f963f654",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34b70254-e322-44ae-bf26-b436f963f654",
    "outputId": "45cdc9c7-5158-4de2-9622-30f4f2c8aaa6"
   },
   "outputs": [],
   "source": [
    "preds_lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eeaa6f-b594-4082-897d-23c4f9d54605",
   "metadata": {
    "id": "77eeaa6f-b594-4082-897d-23c4f9d54605"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"gpt2_1epoch_law.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m47IvKfggdNh",
   "metadata": {
    "id": "m47IvKfggdNh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "021b740c950344d6b2a4b7f1281fa459": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "02e58928c6144ac58b6b9bb186bbb1da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0391aac83d144fd0995a0969660c6d79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0655b51ba11d4538b87335575936bd3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0673d40924ea4f74b4dc33105d460710": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "111f864787a440238cf07312c9485fcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eaaec0280b064f639659772b92d04cd1",
      "placeholder": "​",
      "style": "IPY_MODEL_caf7989d69dc467980c2dcebfa51000a",
      "value": "Downloading (…)okenizer_config.json: 100%"
     }
    },
    "24e02a3a6d5d42c4b7a8c2e61e5b7646": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cfd334db36b8463bb76ba439df3de5c1",
      "placeholder": "​",
      "style": "IPY_MODEL_aa7d7ff4b3754ac7b9c123eddb36313d",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "2858bdcd8f954d118b9c1c29c88bafc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2ce22b73c9494e15a47b90b9e60826a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_506982c663394c2da96cd838bda62521",
      "placeholder": "​",
      "style": "IPY_MODEL_0391aac83d144fd0995a0969660c6d79",
      "value": " 213k/213k [00:00&lt;00:00, 1.34MB/s]"
     }
    },
    "2de368482ca24ee58c5b649b1879d0ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_111f864787a440238cf07312c9485fcb",
       "IPY_MODEL_4253917776994cb8a9d8cd070e13cc6e",
       "IPY_MODEL_f03f4e967c1245a1874d6da3aa75dc78"
      ],
      "layout": "IPY_MODEL_ac722dec80774fb2a54b45d036b8a447"
     }
    },
    "30072a1eb15448caa54986bfcf363508": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "33dcaafdd2154c5787704d727478ab8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "34e7299f511f4c7c9fe2efb430ea6439": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "363086df51154442bea52e5e7e63235c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4002ae2dabcc417193860fa8308a5c0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0655b51ba11d4538b87335575936bd3e",
      "max": 665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fe4e2f83dacb486682a125976bd236f9",
      "value": 665
     }
    },
    "422e115234c8458291c7da7b96d403a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4253917776994cb8a9d8cd070e13cc6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e39667400951427b9116ad5c4d6c0ef1",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4c4e961361cf4bacbef0046146b77f68",
      "value": 29
     }
    },
    "46b80ca60ea1438991b61e1f64568ea9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "476b76aefb8b4772bc6bfb4810ed9f28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c02664529514481899a11aed85c3a2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_778e135f412c411da0ba1a473c8a4b29",
      "placeholder": "​",
      "style": "IPY_MODEL_46b80ca60ea1438991b61e1f64568ea9",
      "value": " 570/570 [00:00&lt;00:00, 41.4kB/s]"
     }
    },
    "4c4e961361cf4bacbef0046146b77f68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "506982c663394c2da96cd838bda62521": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b5d29642d4644fe9973187770685eb2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5ed9fb5dedb946a6bcae3c013c9cbae0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65eff840b15440058b2e79c4d2d2cd57",
      "placeholder": "​",
      "style": "IPY_MODEL_d3a0c2d51d574238a2a03c9ad5aebc24",
      "value": "Downloading (…)solve/main/vocab.txt: 100%"
     }
    },
    "65eff840b15440058b2e79c4d2d2cd57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "677a2145d16f4370b8868090a0418f91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67c6eb1cf61f4760baa63f459ad8849b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_24e02a3a6d5d42c4b7a8c2e61e5b7646",
       "IPY_MODEL_4002ae2dabcc417193860fa8308a5c0c",
       "IPY_MODEL_73af27279ff840f3961ee2871000b829"
      ],
      "layout": "IPY_MODEL_677a2145d16f4370b8868090a0418f91"
     }
    },
    "69d3a45068514682bfcfddb58aa4d7a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5ed9fb5dedb946a6bcae3c013c9cbae0",
       "IPY_MODEL_b6ee4575ecb64be283fecd5c48fd0fb0",
       "IPY_MODEL_2ce22b73c9494e15a47b90b9e60826a7"
      ],
      "layout": "IPY_MODEL_0673d40924ea4f74b4dc33105d460710"
     }
    },
    "73af27279ff840f3961ee2871000b829": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_02e58928c6144ac58b6b9bb186bbb1da",
      "placeholder": "​",
      "style": "IPY_MODEL_33dcaafdd2154c5787704d727478ab8b",
      "value": " 665/665 [00:00&lt;00:00, 43.0kB/s]"
     }
    },
    "7416ce601875494394a31e709b11202b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "778e135f412c411da0ba1a473c8a4b29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7863f87983ac4faebead83e522184028": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e93986f5287404da2efa7216c243d5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_919212ca8dfd4fb0b83acad71fddcb30",
      "placeholder": "​",
      "style": "IPY_MODEL_476b76aefb8b4772bc6bfb4810ed9f28",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "850627ecf7dd442b86923ab556ea9aef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8c5db2d74e744ef3b32997de5778a0cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "919212ca8dfd4fb0b83acad71fddcb30": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91e67c9229164ffc80be89782e794a24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9266fbc9b9684f5cae3b306dbf3d53aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ff15715aa6c048d4a81b558efeb3ad37",
       "IPY_MODEL_cbf6c1e9cb4845229ff98366e16fdf28",
       "IPY_MODEL_a91232f50f324b7fb7642cab5d9b3548"
      ],
      "layout": "IPY_MODEL_34e7299f511f4c7c9fe2efb430ea6439"
     }
    },
    "94de92b07b7d4c1a9f7f5935d7580f0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9516655a12e64704a2148cf13d11635b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "972abc11f9d84247bf307c8ed357af72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97cdab3ccfe04532bdcc17dfc27b479d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c21717e01b854b47a8701d8c9e09ea55",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8c5db2d74e744ef3b32997de5778a0cb",
      "value": 570
     }
    },
    "9becc64a40b348a2880b3fc9e5860d25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c827731d8c854ab0b1cc25a30d21f7d7",
       "IPY_MODEL_b5a809540098474aa2fba2e957d54378",
       "IPY_MODEL_ee20ba466d5745719556e5973268cbb9"
      ],
      "layout": "IPY_MODEL_91e67c9229164ffc80be89782e794a24"
     }
    },
    "9cb6a244f0b842849fb730c5ec240b47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a91232f50f324b7fb7642cab5d9b3548": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9b31f6be2094291b7194793569abd5c",
      "placeholder": "​",
      "style": "IPY_MODEL_30072a1eb15448caa54986bfcf363508",
      "value": " 548M/548M [00:03&lt;00:00, 340MB/s]"
     }
    },
    "aa7d7ff4b3754ac7b9c123eddb36313d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac722dec80774fb2a54b45d036b8a447": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5a809540098474aa2fba2e957d54378": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_363086df51154442bea52e5e7e63235c",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7416ce601875494394a31e709b11202b",
      "value": 124
     }
    },
    "b6ee4575ecb64be283fecd5c48fd0fb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_021b740c950344d6b2a4b7f1281fa459",
      "max": 213450,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2858bdcd8f954d118b9c1c29c88bafc5",
      "value": 213450
     }
    },
    "b75295ab87de4b12a969c6a80de6c2b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7e93986f5287404da2efa7216c243d5a",
       "IPY_MODEL_97cdab3ccfe04532bdcc17dfc27b479d",
       "IPY_MODEL_4c02664529514481899a11aed85c3a2d"
      ],
      "layout": "IPY_MODEL_9cb6a244f0b842849fb730c5ec240b47"
     }
    },
    "c21717e01b854b47a8701d8c9e09ea55": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c62e7e5e5dfa4ae89b886204da4370a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c827731d8c854ab0b1cc25a30d21f7d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d222adfaf766455ca9ab81d890c14f2d",
      "placeholder": "​",
      "style": "IPY_MODEL_850627ecf7dd442b86923ab556ea9aef",
      "value": "Downloading (…)neration_config.json: 100%"
     }
    },
    "caf7989d69dc467980c2dcebfa51000a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cbf6c1e9cb4845229ff98366e16fdf28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_972abc11f9d84247bf307c8ed357af72",
      "max": 548118077,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c62e7e5e5dfa4ae89b886204da4370a8",
      "value": 548118077
     }
    },
    "cfd334db36b8463bb76ba439df3de5c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d222adfaf766455ca9ab81d890c14f2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3a0c2d51d574238a2a03c9ad5aebc24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d9b31f6be2094291b7194793569abd5c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "daf1f14a06e940c091acd2cda8bfdc66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e39667400951427b9116ad5c4d6c0ef1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eaaec0280b064f639659772b92d04cd1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee20ba466d5745719556e5973268cbb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_daf1f14a06e940c091acd2cda8bfdc66",
      "placeholder": "​",
      "style": "IPY_MODEL_422e115234c8458291c7da7b96d403a9",
      "value": " 124/124 [00:00&lt;00:00, 9.59kB/s]"
     }
    },
    "f03f4e967c1245a1874d6da3aa75dc78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7863f87983ac4faebead83e522184028",
      "placeholder": "​",
      "style": "IPY_MODEL_5b5d29642d4644fe9973187770685eb2",
      "value": " 29.0/29.0 [00:00&lt;00:00, 1.68kB/s]"
     }
    },
    "fe4e2f83dacb486682a125976bd236f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ff15715aa6c048d4a81b558efeb3ad37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9516655a12e64704a2148cf13d11635b",
      "placeholder": "​",
      "style": "IPY_MODEL_94de92b07b7d4c1a9f7f5935d7580f0e",
      "value": "Downloading pytorch_model.bin: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
