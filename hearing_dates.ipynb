{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing to Match Annotated Data\n",
    "- Ultimate goal: Match everything as closely as possible (even if it doesn't always make sense to)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Data (Annotated by Partner)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming `gold_data` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['file_number_gold', 'file_number_gold_cleaned', 'raw_file_text',\n",
       "       'raw_file_name', 'timestamp', 'email_address', 'hearing_date',\n",
       "       'decision_date', 'adjudicating_member', 'ltb_location',\n",
       "       'landlord_represented', 'landlord_attended_hearing',\n",
       "       'tenant_represented', 'tenant_attended_hearing', 'landlord_nonprofit',\n",
       "       'tenant_collecting_subsidy', 'case_outcome', 'tenancy_length',\n",
       "       'monthly_rent', 'rental_deposit', 'rent_after_increase',\n",
       "       'rent_increase_effect_date', 'total_arrears', 'arrears_duration',\n",
       "       'arrears_payment_amount', 'tenant_arrears_history_mentioned',\n",
       "       'tenant_arrears_payment_history_mentioned',\n",
       "       'rent_payments_late_frequency', 'tenant_ability_to_pay_rent',\n",
       "       'tenant_conditions', 'tenant_children_present', 'total_children',\n",
       "       'children_17_or_younger', 'children_13_or_younger',\n",
       "       'children_4_or_younger', 'children_conditions_mentioned',\n",
       "       'conditions_making_moving_burdensome', 'tenant_employed',\n",
       "       'tenant_government_assistance', 'employment_stability_doubts',\n",
       "       'sufficient_income_to_pay_rent', 'total_household_income',\n",
       "       'tenant_job_loss_mentioned', 'tenant_extenuating_circumstances',\n",
       "       'tenant_proposed_payment_plan', 'accepted_proposed_payment_plan',\n",
       "       'payment_plan_length', 'tenant_difficulty_finding_housing',\n",
       "       'applicable_difficulty_reasons', 'tenant_prior_notice_given',\n",
       "       'prior_notice_duration', 'postponement_additional_arrears',\n",
       "       'mentioned_applications', 'validity_of_N4_notice_mentioned',\n",
       "       'additional_details_in_decision', 'executive_review', 'review_status'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_data = pd.read_csv(\"data/gold_labels_with_files.csv\")\n",
    "\n",
    "# making new names for the columns in gold_data\n",
    "\n",
    "new_names = {\n",
    "    'Timestamp': 'timestamp',\n",
    "    'Email Address': 'email_address',\n",
    "    'What is the file number of the case?': 'file_number_gold',\n",
    "    'What was the date of the hearing? [mm/dd/yyyy]': 'hearing_date',\n",
    "    'What was the date of the decision? [mm/dd/yyyy]': 'decision_date',\n",
    "    'Who was the member adjudicating the decision?': 'adjudicating_member',\n",
    "    'What was the location of the landlord tenant board?': 'ltb_location',\n",
    "    'Did the decision state the landlord was represented?': 'landlord_represented',\n",
    "    'Did the decision state the landlord attended the hearing?': 'landlord_attended_hearing',\n",
    "    'Did the decision state the tenant was represented?': 'tenant_represented',\n",
    "    'Did the decision state the tenant attended the hearing?': 'tenant_attended_hearing',\n",
    "    'Did the decision state the landlord was a not-for-profit landlord (e.g. Toronto Community Housing)?': 'landlord_nonprofit',\n",
    "    'Did the decision state the tenant was collecting a subsidy?': 'tenant_collecting_subsidy',\n",
    "    'What was the outcome of the case?': 'case_outcome',\n",
    "    'What was the length of the tenancy, or in other words, how long had the tenants lived at the residence in question? ': 'tenancy_length',\n",
    "    'What was the monthly rent?': 'monthly_rent',\n",
    "    'What was the amount of the rental deposit? ': 'rental_deposit',\n",
    "    'If any rent increases occurred, what was the rent after the increase(s)?': 'rent_after_increase',\n",
    "    'If any rent increases occurred, when did the rent increase(s) come into effect? ': 'rent_increase_effect_date',\n",
    "    'What was the total amount of arrears?': 'total_arrears',\n",
    "    'Over how many months did the arrears accumulate? ': 'arrears_duration',\n",
    "    'If the tenant made a payment on the arrears after the eviction notice was served and/or prior to the hearing, what was the amount of the payment? ': 'arrears_payment_amount',\n",
    "    'Did the decision mention a history of arrears by the tenant separate from the arrears in the current claim (more than one period of arrears, recurrently coming in and out of arrears, arrears with previous landlord, etc.)?': 'tenant_arrears_history_mentioned',\n",
    "    'If the tenant had a history of arrears, did the decision mention a history of the tenant making payments on those arrears (separate from any payments made in response to the present eviction notice/hearing)?': 'tenant_arrears_payment_history_mentioned',\n",
    "    'How frequently were rent payments made late?': 'rent_payments_late_frequency',\n",
    "    'Did the member find the tenant had or seemed to have the ability to pay rent, but chose not do so?': 'tenant_ability_to_pay_rent',\n",
    "    'What were the specific mental, medical, or physical conditions of the tenant, if any? ': 'tenant_conditions',\n",
    "    'Did the decision state that the tenant had children living with them?': 'tenant_children_present',\n",
    "    'How many total children did the tenant have living with them? ': 'total_children',\n",
    "    'How many total children aged 17 or younger did the tenant have living with them?': 'children_17_or_younger',\n",
    "    'How many total children aged 13 or younger did the tenant have living with them? ': 'children_13_or_younger',\n",
    "    'How many total children aged 4 or younger did the tenant have living with them?': 'children_4_or_younger',\n",
    "    'Did the decision state any of the children had mental, medical or physical conditions?': 'children_conditions_mentioned',\n",
    "    'If yes to the previous question, did the decision state these conditions would make moving particularly burdensome?': 'conditions_making_moving_burdensome',\n",
    "    'Was the tenant employed at the time of the hearing?': 'tenant_employed',\n",
    "    'If the tenant was not employed, did the decision state the tenant was receiving any form of government assistance (e.g. OW, childcare benefits, ODSP, OSAP)?': 'tenant_government_assistance',\n",
    "    'If the tenant was employed, did the decision state any doubts about the stability of employment e.g. lack of guaranteed hours, contract work, etc.?': 'employment_stability_doubts',\n",
    "    'Did the member find the tenant had sufficient income to pay rent?': 'sufficient_income_to_pay_rent',\n",
    "    'What was the total income of the tenant’s household? ': 'total_household_income',\n",
    "    'Did the decision mention the tenant lost their job leading up to or during the period of the hearing?': 'tenant_job_loss_mentioned',\n",
    "    'Did the decision mention any other extenuating circumstances experienced by the tenant leading up to or during the period of the claim (e.g. hospitalization, death in the family, etc.)?': 'tenant_extenuating_circumstances',\n",
    "    'Did the tenant propose a payment plan?': 'tenant_proposed_payment_plan',\n",
    "    'If the tenant did propose a payment plan, did the member accept the proposed payment plan?': 'accepted_proposed_payment_plan',\n",
    "    'If a payment plan was ordered, what was the length of the payment plan? ': 'payment_plan_length',\n",
    "    'Did the decision mention the tenant’s difficulty finding alternative housing for any reason e.g.physical limitations, reliance on social assistance, etc.?': 'tenant_difficulty_finding_housing',\n",
    "    'If yes to the previous question, which of the following were applicable to the tenant?': 'applicable_difficulty_reasons',\n",
    "    'Did the decision state the tenant was given prior notice for the eviction?': 'tenant_prior_notice_given',\n",
    "    'If the tenant was given prior notice for the eviction, how much notice was given?': 'prior_notice_duration',\n",
    "    'Did the decisions state postponement would result in the tenant accruing additional arrears?': 'postponement_additional_arrears',\n",
    "    'Which other specific applications of the landlord or the tenant were mentioned?': 'mentioned_applications',\n",
    "    'Did the decision mention the validity of an N4 eviction notice?': 'validity_of_N4_notice_mentioned',\n",
    "    'Were there detail(s) in the decision not captured by this questionnaire that should be included?': 'additional_details_in_decision',\n",
    "    'Exec Review': 'executive_review',\n",
    "    'Review Status': 'review_status'\n",
    "}\n",
    "\n",
    "gold_data = gold_data.rename(columns = new_names)\n",
    "# sorting by file_number -- so that ordering of the new data annotations can match this and be more easily compared\n",
    "gold_data = gold_data.sort_values(by = ['file_number_gold'], ascending = True).reset_index(drop = True)\n",
    "gold_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def general_cleaning(raw_file_str: str):\n",
    "    # gets rid of tabs, non-breaking spaces, leading/trailing whitespace, removes empty lines, and \"\\xa0\"\n",
    "    generally_cleaned_str = [line.replace(\"\\t\", \" \").replace(\"\\xa0\", \"\").strip() for line in raw_file_str.split('\\n') if line.strip() != '']\n",
    "    return generally_cleaned_str\n",
    "\n",
    "def remove_whitespace_and_underscores(string):\n",
    "    # Remove consecutive whitespace\n",
    "    string = re.sub(r'\\s+', ' ', string)\n",
    "\n",
    "    # Remove more than three consecutive underscores\n",
    "    string = re.sub(r'_+', '', string)\n",
    "\n",
    "    return string.strip()\n",
    "\n",
    "def separate_file_sections(text_list):\n",
    "    metadata_list = []\n",
    "    content_list = []\n",
    "\n",
    "    is_metadata = True\n",
    "    is_content = False\n",
    "\n",
    "    for line in text_list:\n",
    "        if line.strip() == 'Metadata:':\n",
    "            is_metadata = True\n",
    "            is_content = False\n",
    "        elif line.strip() == 'Content:':\n",
    "            is_metadata = False\n",
    "            is_content = True\n",
    "        elif is_metadata:\n",
    "            metadata_list.append(remove_whitespace_and_underscores(line))\n",
    "        elif is_content:\n",
    "            content_list.append(remove_whitespace_and_underscores(line))\n",
    "\n",
    "    return metadata_list, content_list\n",
    "\n",
    "def merge_numerical_entries(strings_list):\n",
    "    \"\"\"\n",
    "    Turns something like\n",
    "        [..., '3.',\n",
    "        'The tenant took occupancy of the rental unit in or about the beginning of December 2016.', ...]\n",
    "    into\n",
    "        [..., '3. The tenant took occupancy of the rental unit in or about the beginning of December 2016.', ...]\n",
    "    \n",
    "    \"\"\"\n",
    "    for i in range(len(strings_list) - 2, -1, -1):\n",
    "        if re.fullmatch(r'\\d+\\.', strings_list[i]):\n",
    "            strings_list[i] += ' ' + strings_list[i + 1]\n",
    "            del strings_list[i + 1]\n",
    "    return strings_list\n",
    "\n",
    "def move_trailing_numbers(strings_list):\n",
    "    \"\"\"\n",
    "    Turns something like\n",
    "        [..., 'Credibility of the Parties 4.',\n",
    "        'The Landlord said about two to three months ago he ...', ...]\n",
    "    into\n",
    "        [..., 'Credibility of the Parties',\n",
    "        '4. The Landlord said about two to three months ago he...', ...]\n",
    "    \n",
    "    \"\"\"\n",
    "    for i in range(len(strings_list) - 1, -1, -1):\n",
    "        match = re.search(r'\\s+(\\d{2}\\.)$', strings_list[i])\n",
    "        if match:\n",
    "            number = match.group(1)\n",
    "            strings_list[i] = re.sub(r'\\s+\\d{1,2}\\.$', '', strings_list[i])\n",
    "            strings_list[i + 1] = number + ' ' + strings_list[i + 1]\n",
    "    return strings_list\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_end_tag_and_restructure(metadata_list: list):\n",
    "\n",
    "    cleaned_str = \" \".join(metadata_list)\n",
    "\n",
    "    # this doesn't add any meaning to the case details we need to extract, and instead just adds noise to the extraction process + adds extra unnecessary tokens\n",
    "    if cleaned_str.find(\"If you have any questions about this order\") > (len(cleaned_str) - 500):\n",
    "        cleaned_str = cleaned_str[: cleaned_str.find(\"If you have any questions about this order\")].strip() # ending tag removed\n",
    "    \n",
    "    # otherwise just do everything else\n",
    "    cleaned_str = cleaned_str.replace(\". \", \".\\n\")\n",
    "    # cleaned_str = cleaned_str.replace(\". \", \".\\n\") # deprecated by regex approach\n",
    "    # trimmed_list = [line.strip() for line in re.split(r'(?<!\\d)\\. ', cleaned_str) if line.strip() != ''] # deprecated by regex approach\n",
    "    cleaned_str = re.sub(r'(?<!\\d)\\. ', \"\\n\", cleaned_str)\n",
    "    trimmed_list = [line.strip() for line in cleaned_str.split('\\n') if line.strip() != '']\n",
    "    trimmed_list = merge_numerical_entries(trimmed_list)\n",
    "    trimmed_list = move_trailing_numbers(trimmed_list)\n",
    "    return trimmed_list\n",
    "\n",
    "# file_name = \"CEL-74519-18.txt\"\n",
    "# # row of this particular case\n",
    "# case_file_ind = silver_data.loc[silver_data['raw_file_name'] == file_name].index.tolist()[0]\n",
    "# test_text = silver_data.loc[206, \"raw_file_text\"]#.item()\n",
    "\n",
    "# metadata, content = separate_file_sections(general_cleaning(test_text))\n",
    "# remove_end_tag_and_restructure(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files processed: 679 of 679, Estimated time remaining: 00:00:00\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_number_gold</th>\n",
       "      <th>file_number</th>\n",
       "      <th>raw_file_text</th>\n",
       "      <th>raw_file_name</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>email_address</th>\n",
       "      <th>hearing_date</th>\n",
       "      <th>decision_date</th>\n",
       "      <th>adjudicating_member</th>\n",
       "      <th>ltb_location</th>\n",
       "      <th>...</th>\n",
       "      <th>prior_notice_duration</th>\n",
       "      <th>postponement_additional_arrears</th>\n",
       "      <th>mentioned_applications</th>\n",
       "      <th>validity_of_N4_notice_mentioned</th>\n",
       "      <th>additional_details_in_decision</th>\n",
       "      <th>executive_review</th>\n",
       "      <th>review_status</th>\n",
       "      <th>full_cleaned</th>\n",
       "      <th>metadata</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SWL-17348-18</td>\n",
       "      <td>SWL-17348-18</td>\n",
       "      <td>Metadata:\\nDate:\\t2018-07-06\\nFile number:\\t\\n...</td>\n",
       "      <td>SWL-17348-18.txt</td>\n",
       "      <td>11/26/2020 13:40:11</td>\n",
       "      <td>johnnymetzger6@gmail.com</td>\n",
       "      <td>07/05/2018</td>\n",
       "      <td>07/06/2018</td>\n",
       "      <td>Kevin Lundy</td>\n",
       "      <td>London</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>No</td>\n",
       "      <td>L1: Application to Evict a Tenant for Non-paym...</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AW</td>\n",
       "      <td>Complete</td>\n",
       "      <td>[Metadata:, Date: 2018-07-06, File number:, SW...</td>\n",
       "      <td>[Date: 2018-07-06, File number:, SWL-17348-18,...</td>\n",
       "      <td>[Order under Section 69 Residential Tenancies ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEL-79722-17</td>\n",
       "      <td>TEL-79722-17</td>\n",
       "      <td>Metadata:\\nDate:\\t2017-05-26\\nFile number:\\t\\n...</td>\n",
       "      <td>TEL-79722-17.txt</td>\n",
       "      <td>1/31/2021 19:55:04</td>\n",
       "      <td>griffin.murphy@mail.utoronto.ca</td>\n",
       "      <td>05/19/2017</td>\n",
       "      <td>05/26/2017</td>\n",
       "      <td>Laura Hartslief</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Yes</td>\n",
       "      <td>L1: Application to Evict a Tenant for Non-paym...</td>\n",
       "      <td>No</td>\n",
       "      <td>Tenant’s conduct (racial slurs, aggressive beh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Metadata:, Date: 2017-05-26, File number:, TE...</td>\n",
       "      <td>[Date: 2017-05-26, File number:, TEL-79722-17,...</td>\n",
       "      <td>[Order under Section 69 Residential Tenancies ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEL-80773-17</td>\n",
       "      <td>TEL-80773-17</td>\n",
       "      <td>Metadata:\\nDate:\\t2017-07-05\\nFile number:\\t\\n...</td>\n",
       "      <td>TEL-80773-17.txt</td>\n",
       "      <td>1/31/2021 21:08:55</td>\n",
       "      <td>griffin.murphy@mail.utoronto.ca</td>\n",
       "      <td>06/30/2017</td>\n",
       "      <td>07/05/2017</td>\n",
       "      <td>Ruth Carey</td>\n",
       "      <td>Whitby</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No other specific applications were mentioned</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Metadata:, Date: 2017-07-05, File number:, TE...</td>\n",
       "      <td>[Date: 2017-07-05, File number:, TEL-80773-17,...</td>\n",
       "      <td>[Order under Section 69 Residential Tenancies ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEL-81359-17-AM</td>\n",
       "      <td>TEL-81359-17-AM</td>\n",
       "      <td>Metadata:\\nDate:\\t2017-07-18\\nFile number:\\t\\n...</td>\n",
       "      <td>TEL-81359-17-AM.txt</td>\n",
       "      <td>1/31/2021 21:21:33</td>\n",
       "      <td>griffin.murphy@mail.utoronto.ca</td>\n",
       "      <td>07/07/2017</td>\n",
       "      <td>07/18/2017</td>\n",
       "      <td>Shelby Whittick</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Yes</td>\n",
       "      <td>L1: Application to Evict a Tenant for Non-paym...</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Metadata:, Date: 2017-07-18, File number:, TE...</td>\n",
       "      <td>[Date: 2017-07-18, File number:, TEL-81359-17-...</td>\n",
       "      <td>[Order under Section 69 Residential Tenancies ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEL-81405-17</td>\n",
       "      <td>TEL-81405-17</td>\n",
       "      <td>Metadata:\\nDate:\\t2017-08-17\\nFile number:\\t\\n...</td>\n",
       "      <td>TEL-81405-17.txt</td>\n",
       "      <td>1/31/2021 21:32:22</td>\n",
       "      <td>griffin.murphy@mail.utoronto.ca</td>\n",
       "      <td>08/15/2017</td>\n",
       "      <td>08/17/2017</td>\n",
       "      <td>Laura Hartslief</td>\n",
       "      <td>Lindsay</td>\n",
       "      <td>...</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No other specific applications were mentioned</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Metadata:, Date: 2017-08-17, File number:, TE...</td>\n",
       "      <td>[Date: 2017-08-17, File number:, TEL-81405-17,...</td>\n",
       "      <td>[Order under Section 69 Residential Tenancies ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_number_gold      file_number  \\\n",
       "0      SWL-17348-18     SWL-17348-18   \n",
       "1      TEL-79722-17     TEL-79722-17   \n",
       "2      TEL-80773-17     TEL-80773-17   \n",
       "3   TEL-81359-17-AM  TEL-81359-17-AM   \n",
       "4      TEL-81405-17     TEL-81405-17   \n",
       "\n",
       "                                       raw_file_text         raw_file_name  \\\n",
       "0  Metadata:\\nDate:\\t2018-07-06\\nFile number:\\t\\n...      SWL-17348-18.txt   \n",
       "1  Metadata:\\nDate:\\t2017-05-26\\nFile number:\\t\\n...      TEL-79722-17.txt   \n",
       "2  Metadata:\\nDate:\\t2017-07-05\\nFile number:\\t\\n...      TEL-80773-17.txt   \n",
       "3  Metadata:\\nDate:\\t2017-07-18\\nFile number:\\t\\n...   TEL-81359-17-AM.txt   \n",
       "4  Metadata:\\nDate:\\t2017-08-17\\nFile number:\\t\\n...      TEL-81405-17.txt   \n",
       "\n",
       "             timestamp                    email_address hearing_date  \\\n",
       "0  11/26/2020 13:40:11         johnnymetzger6@gmail.com   07/05/2018   \n",
       "1   1/31/2021 19:55:04  griffin.murphy@mail.utoronto.ca   05/19/2017   \n",
       "2   1/31/2021 21:08:55  griffin.murphy@mail.utoronto.ca   06/30/2017   \n",
       "3   1/31/2021 21:21:33  griffin.murphy@mail.utoronto.ca   07/07/2017   \n",
       "4   1/31/2021 21:32:22  griffin.murphy@mail.utoronto.ca   08/15/2017   \n",
       "\n",
       "  decision_date adjudicating_member ltb_location  ... prior_notice_duration  \\\n",
       "0    07/06/2018         Kevin Lundy       London  ...            Not stated   \n",
       "1    05/26/2017     Laura Hartslief      Toronto  ...            Not stated   \n",
       "2    07/05/2017          Ruth Carey       Whitby  ...            Not stated   \n",
       "3    07/18/2017     Shelby Whittick      Toronto  ...            Not stated   \n",
       "4    08/17/2017     Laura Hartslief      Lindsay  ...            Not stated   \n",
       "\n",
       "  postponement_additional_arrears  \\\n",
       "0                              No   \n",
       "1                             Yes   \n",
       "2                             Yes   \n",
       "3                             Yes   \n",
       "4                             Yes   \n",
       "\n",
       "                              mentioned_applications  \\\n",
       "0  L1: Application to Evict a Tenant for Non-paym...   \n",
       "1  L1: Application to Evict a Tenant for Non-paym...   \n",
       "2      No other specific applications were mentioned   \n",
       "3  L1: Application to Evict a Tenant for Non-paym...   \n",
       "4      No other specific applications were mentioned   \n",
       "\n",
       "  validity_of_N4_notice_mentioned  \\\n",
       "0                              No   \n",
       "1                              No   \n",
       "2                              No   \n",
       "3                              No   \n",
       "4                              No   \n",
       "\n",
       "                      additional_details_in_decision executive_review  \\\n",
       "0                                                NaN               AW   \n",
       "1  Tenant’s conduct (racial slurs, aggressive beh...              NaN   \n",
       "2                                                NaN              NaN   \n",
       "3                                                NaN              NaN   \n",
       "4                                                NaN              NaN   \n",
       "\n",
       "  review_status                                       full_cleaned  \\\n",
       "0      Complete  [Metadata:, Date: 2018-07-06, File number:, SW...   \n",
       "1           NaN  [Metadata:, Date: 2017-05-26, File number:, TE...   \n",
       "2           NaN  [Metadata:, Date: 2017-07-05, File number:, TE...   \n",
       "3           NaN  [Metadata:, Date: 2017-07-18, File number:, TE...   \n",
       "4           NaN  [Metadata:, Date: 2017-08-17, File number:, TE...   \n",
       "\n",
       "                                            metadata  \\\n",
       "0  [Date: 2018-07-06, File number:, SWL-17348-18,...   \n",
       "1  [Date: 2017-05-26, File number:, TEL-79722-17,...   \n",
       "2  [Date: 2017-07-05, File number:, TEL-80773-17,...   \n",
       "3  [Date: 2017-07-18, File number:, TEL-81359-17-...   \n",
       "4  [Date: 2017-08-17, File number:, TEL-81405-17,...   \n",
       "\n",
       "                                             content  \n",
       "0  [Order under Section 69 Residential Tenancies ...  \n",
       "1  [Order under Section 69 Residential Tenancies ...  \n",
       "2  [Order under Section 69 Residential Tenancies ...  \n",
       "3  [Order under Section 69 Residential Tenancies ...  \n",
       "4  [Order under Section 69 Residential Tenancies ...  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize a deque to store the latest 100 iteration times\n",
    "time_deque = deque(maxlen = 500)\n",
    "\n",
    "cases_contents = []\n",
    "cases_metadata = []\n",
    "full_cleaned = []\n",
    "\n",
    "raw_files = gold_data['raw_file_text'].tolist()\n",
    "for index, raw_file in enumerate(raw_files):\n",
    "    iteration_start_time = time.time()\n",
    "    better_file = general_cleaning(raw_file)\n",
    "    try:\n",
    "        metadata_list, content_list = separate_file_sections(better_file)\n",
    "        full_cleaned.append(better_file)\n",
    "        # cases_metadata.append(remove_end_tag_and_restructure(metadata_list)) # removing a bit more text if possible\n",
    "        cases_metadata.append(metadata_list) # removing a bit more text if possible\n",
    "        cases_contents.append(remove_end_tag_and_restructure(content_list))\n",
    "\n",
    "        # Save the end time of this iteration and push it into the deque\n",
    "        iteration_end_time = time.time()\n",
    "        time_deque.append(iteration_end_time - iteration_start_time)\n",
    "\n",
    "        # progress tracker\n",
    "        average_time_per_file = np.mean(time_deque)\n",
    "        files_left = len(raw_files) - (index + 1)\n",
    "        estimated_time_left = files_left * average_time_per_file\n",
    "\n",
    "        print(f\"Files processed: {index + 1} of {len(raw_files)}, Estimated time remaining: {time.strftime('%H:%M:%S', time.gmtime(estimated_time_left))}\", end='\\r')\n",
    "    except Exception as any_error:\n",
    "        print(f\"{any_error} with file at Df row: \", index)\n",
    "\n",
    "gold_data['full_cleaned'] = full_cleaned\n",
    "gold_data['metadata'] = cases_metadata\n",
    "gold_data['content'] = cases_contents\n",
    "gold_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nant remained in the unit after the termination\\ndate (the ‘L2 Application’).\\n\\xa0\\nThese applications were heard in [CITY] on\\nJuly 5, 2018.\\xa0 Only the Land'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_data.loc[0, 'raw_file_text'][700:850]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 14:30:19.145979: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'07/05/2018'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import spacy\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\") # loading this outside of the function saves ~2s per function call\n",
    "\n",
    "# def find_dates_in_string(string, model=nlp):\n",
    "#     extracted_dates = []\n",
    "#     # for string in string_list:\n",
    "#     doc = nlp(string)\n",
    "#     for entity in doc.ents:\n",
    "#         if entity.label_ == \"DATE\":\n",
    "#             extracted_dates.append(entity.text)\n",
    "    \n",
    "#     return list(set(extracted_dates))\n",
    "#     # pattern = r\"(?i)(\\b\\w+ \\d{1,2}, \\d{4}\\b)\"\n",
    "#     # valid_dates = re.findall(pattern, \", \".join(extracted_dates))\n",
    "#     # return valid_dates\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def order_dates(dates_list):\n",
    "    sorted_dates = sorted(dates_list, key=lambda x: datetime.strptime(x, '%m/%d/%Y'))\n",
    "    return sorted_dates\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "def convert_to_datetime(date_str):\n",
    "    # Parse date using dateutil.parser.parse\n",
    "    dt = parse(date_str)\n",
    "    \n",
    "    # Format date with strftime in the format 'MM/DD/YYYY'\n",
    "    return dt.strftime('%m/%d/%Y')\n",
    "\n",
    "gold_data['hearing_date'] = gold_data['hearing_date'].apply(lambda x: convert_to_datetime(x))\n",
    "gold_data['decision_date'] = gold_data['decision_date'].apply(lambda x: convert_to_datetime(x))\n",
    "gold_data.loc[0, 'hearing_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'July 07, 2017'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def convert_date_to_words(date_str):\n",
    "    date = datetime.strptime(date_str, '%m/%d/%Y')\n",
    "    formatted_date = date.strftime('%B %d, %Y')\n",
    "    return formatted_date\n",
    "\n",
    "convert_date_to_words(gold_data.loc[3, 'hearing_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_dates_regex(string):\n",
    "    date_pattern = r'\\b\\w+ \\d{1,2}, \\d{4}\\b'\n",
    "    dates = re.findall(date_pattern, string)\n",
    "    return list(set(dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_words(string):\n",
    "    words = string.split()\n",
    "    word_counts = Counter(words)\n",
    "    return word_counts\n",
    "    # return dict(word_counts)\n",
    "\n",
    "def sort_dict_by_values(dictionary):\n",
    "    sorted_dict = dict(sorted(dictionary.items(), key=lambda item: item[1]))\n",
    "    return sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_punctuation(word):\n",
    "    cleaned_word = re.sub(r'^\\W+|\\W+$', '', word)\n",
    "    return cleaned_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.8333333333333335\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def cosine_similarity(dict1, dict2):\n",
    "    common_words = set(dict1.keys()).intersection(set(dict2.keys()))\n",
    "    common_words = set(dict1).intersection(set(dict2))\n",
    "\n",
    "    dot_product = sum(dict1[word] * dict2[word] for word in common_words)\n",
    "    magnitude1 = math.sqrt(sum(dict1[word]**2 for word in dict1))\n",
    "    magnitude2 = math.sqrt(sum(dict2[word]**2 for word in dict2))\n",
    "\n",
    "    # if somehow this ever happens, return 0 instead of throwing an error\n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0\n",
    "\n",
    "    similarity = dot_product / (magnitude1 * magnitude2)\n",
    "    return similarity\n",
    "\n",
    "# Example usage\n",
    "case_file1 = \"This is the first case file.\"\n",
    "case_file2 = \"This is the seconds case file.\"\n",
    "\n",
    "words_counts1 = count_words(case_file1.lower())#.most_common(20)\n",
    "words_counts2 = count_words(case_file2.lower())#.most_common(20)\n",
    "\n",
    "similarity = cosine_similarity(words_counts1, words_counts2)\n",
    "print(f\"Similarity: {similarity}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONT DELETE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample sentence , showing stop words filtration .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kmaurinjones/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kmaurinjones/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# You need to download the set of stop words the first time\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(text, stopwords = stopwords):\n",
    "    word_tokens = word_tokenize(text)\n",
    "\n",
    "    filtered_text = [word for word in word_tokens if word.casefold() not in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "text = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "\n",
    "print(remove_stopwords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6435\n",
      "4625\n"
     ]
    }
   ],
   "source": [
    "test_row = 0\n",
    "test_case_file_str = \" \".join((gold_data.loc[test_row, 'metadata'] + gold_data.loc[test_row, 'content']))#.lower()\n",
    "# test_case_file_str = \" \".join(gold_data.loc[test_row, 'full_cleaned'])#.lower()\n",
    "print(len(test_case_file_str))\n",
    "print(len(remove_stopwords(test_case_file_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/03/2020\n",
      "\n",
      "03/03/2020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_word_dist(case_file_str: str, date_str: str, proximity: int = 100, num_words = 30):\n",
    "\n",
    "    # hearing_date_words = convert_date_to_words(date_str) # retrieved dates from regex expression\n",
    "    hdate_idx = case_file_str.find(date_str) # finds date in case file, gets its starting index\n",
    "    if hdate_idx == -1:\n",
    "        return None\n",
    "    else:\n",
    "        hdate_subset = case_file_str[hdate_idx - proximity : hdate_idx + proximity] # gets case file text within 200 (before and after) hearing date mention\n",
    "        words_counts = count_words(hdate_subset.lower()).most_common(num_words)\n",
    "        cleaned_word_counts = [(remove_punctuation(word), count) for word, count in words_counts]\n",
    "        return dict(cleaned_word_counts)\n",
    "\n",
    "def get_hearing_date_cssim(case_file_str: str):\n",
    "    found_dates = extract_dates_regex(case_file_str)\n",
    "\n",
    "    # print(f\"GOLD DATE: {gold_data.loc[test_row, 'hearing_date']}\\n\")\n",
    "    # print(found_dates)\n",
    "\n",
    "    best_similarity = -np.inf # setting similarity to nothing to start\n",
    "    best_date_candidate = None\n",
    "\n",
    "    for date in found_dates:\n",
    "        word_dist = get_word_dist(case_file_str = case_file_str, date_str = date, proximity = 100)\n",
    "        csim = cosine_similarity(word_dist, dict(master_hdate_dist))\n",
    "        if csim > best_similarity:\n",
    "            best_similarity = csim\n",
    "            best_date_candidate = date\n",
    "\n",
    "    return best_date_candidate\n",
    "\n",
    "test_row = 21\n",
    "test_case_file_str_1 = \" \".join(gold_data.loc[test_row, 'full_cleaned'])\n",
    "print(convert_to_datetime(get_hearing_date_cssim(test_case_file_str_1)))\n",
    "print()\n",
    "test_case_file_str_2 = gold_data.loc[test_row, 'raw_file_text']\n",
    "print(convert_to_datetime(get_hearing_date_cssim(test_case_file_str_2)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 5158),\n",
       " ('in', 1315),\n",
       " ('on', 1185),\n",
       " ('tenant', 1185),\n",
       " ('was', 1103),\n",
       " ('heard', 1036),\n",
       " ('application', 924),\n",
       " ('this', 896),\n",
       " ('attended', 893),\n",
       " ('and', 892),\n",
       " ('hearing', 813),\n",
       " ('2018', 703),\n",
       " ('landlord’s', 618),\n",
       " ('landlord', 586),\n",
       " ('representative', 570),\n",
       " ('toronto', 543),\n",
       " ('tenants', 519),\n",
       " ('rent', 472),\n",
       " ('that', 430),\n",
       " ('not', 419),\n",
       " ('pay', 374),\n",
       " ('did', 365),\n",
       " ('only', 290),\n",
       " ('date', 261),\n",
       " ('termination', 258),\n",
       " ('determinations', 252),\n",
       " ('after', 246),\n",
       " ('unit', 245),\n",
       " ('of', 245),\n",
       " ('owes', 235)]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_row = 10\n",
    "master_hdate_dist = Counter()\n",
    "for row in gold_data.index:\n",
    "    # test_case_file_str = gold_data.loc[row, 'raw_file_text'].lower()\n",
    "    test_case_file_str = \" \".join(gold_data.loc[row, 'full_cleaned']).lower()\n",
    "    # test_case_file_str = remove_stopwords(test_case_file_str)\n",
    "    # hdate = gold_data.loc[row, 'decision_date']\n",
    "    hdate = gold_data.loc[row, 'hearing_date']\n",
    "    hdate_words = convert_date_to_words(hdate).lower()\n",
    "    for hdate_word in [hdate_words, hdate_words.replace(\" 0\", \" \")]: # sometimes they put the 0 in front of the day, sometimes not -- very annoying but need this to be robust\n",
    "        if hdate_word in test_case_file_str: # try with and without \"0\" padding\n",
    "            # print(hdate_word)\n",
    "            dist = get_word_dist(case_file_str = test_case_file_str, date_str = hdate_word, proximity = 100, num_words = 30)\n",
    "            # print(dist)\n",
    "            for word, count in dist.items():\n",
    "                if word in master_hdate_dist:\n",
    "                    master_hdate_dist[word] += count\n",
    "                else:\n",
    "                    master_hdate_dist[word] = count\n",
    "\n",
    "master_hdate_dist = master_hdate_dist.most_common(30)\n",
    "master_hdate_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'July 05, 2018'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_data.loc[0, 'raw_file_text']\n",
    "convert_date_to_words(gold_data.loc[0, 'hearing_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOLD DATE: 03/22/2018\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'March 22, 2018'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_row = 10\n",
    "test_case_file_str = gold_data.loc[test_row, 'raw_file_text']\n",
    "get_hearing_date_cssim(test_case_file_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files processed:  679 of 679 Estimated time remaining:  00:00:00\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hearing_date_raw_text</th>\n",
       "      <th>hearing_date_cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>07/07/2018</td>\n",
       "      <td>07/17/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>06/06/2017</td>\n",
       "      <td>06/06/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>07/16/2017</td>\n",
       "      <td>05/29/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>07/07/2017</td>\n",
       "      <td>07/07/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08/15/2017</td>\n",
       "      <td>08/15/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>11/20/2018</td>\n",
       "      <td>11/20/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>11/23/2018</td>\n",
       "      <td>12/10/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>12/04/2018</td>\n",
       "      <td>12/04/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>11/02/2018</td>\n",
       "      <td>11/02/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>10/15/2019</td>\n",
       "      <td>09/03/2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>679 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    hearing_date_raw_text hearing_date_cleaned_text\n",
       "0              07/07/2018                07/17/2018\n",
       "1              06/06/2017                06/06/2017\n",
       "2              07/16/2017                05/29/2017\n",
       "3              07/07/2017                07/07/2017\n",
       "4              08/15/2017                08/15/2017\n",
       "..                    ...                       ...\n",
       "674            11/20/2018                11/20/2018\n",
       "675            11/23/2018                12/10/2018\n",
       "676            12/04/2018                12/04/2018\n",
       "677            11/02/2018                11/02/2018\n",
       "678            10/15/2019                09/03/2019\n",
       "\n",
       "[679 rows x 2 columns]"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# about 13 per second\n",
    "import time\n",
    "\n",
    "new_data = {}\n",
    "new_data['hearing_date_raw_text'] = []\n",
    "new_data['hearing_date_cleaned_text'] = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize a deque to store the latest 100 iteration times\n",
    "time_deque = deque(maxlen = 500)\n",
    "\n",
    "for index, row in enumerate(gold_data.itertuples()):\n",
    "\n",
    "    # Save the start time of this iteration\n",
    "    iteration_start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        test_case_file_str = gold_data.loc[row.Index, 'raw_file_text']\n",
    "        found_date = get_hearing_date_cssim(test_case_file_str)\n",
    "        if found_date:\n",
    "            found_date = convert_to_datetime(found_date)\n",
    "\n",
    "        new_data['hearing_date_raw_text'].append(found_date) # this way it'll either convert it or add the null value\n",
    "\n",
    "        test_case_file_str = \" \".join(gold_data.loc[row.Index, 'full_cleaned'])\n",
    "        found_date = get_hearing_date_cssim(test_case_file_str)\n",
    "        if found_date:\n",
    "            found_date = convert_to_datetime(found_date)\n",
    "\n",
    "        new_data['hearing_date_cleaned_text'].append(found_date)\n",
    "\n",
    "    except Exception as any_error:\n",
    "        print(f\"{any_error} with file at Df row: \", row.Index)\n",
    "\n",
    "    # Save the end time of this iteration and push it into the deque\n",
    "    iteration_end_time = time.time()\n",
    "    time_deque.append(iteration_end_time - iteration_start_time)\n",
    "\n",
    "    # progress tracker\n",
    "    average_time_per_row = np.mean(time_deque)\n",
    "    rows_left = len(gold_data) - (index + 1)\n",
    "    estimated_time_left = rows_left * average_time_per_row\n",
    "\n",
    "    print(\"Files processed: \", index + 1, \"of\", len(gold_data),\n",
    "          \"Estimated time remaining: \", time.strftime('%H:%M:%S', time.gmtime(estimated_time_left)), end = '\\r')\n",
    "\n",
    "new_df = pd.DataFrame(new_data)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5625920471281296\n",
      "0.7422680412371134\n",
      "07/05/2018 07/17/2018\n",
      "05/19/2017 06/06/2017\n",
      "06/30/2017 05/29/2017\n",
      "01/19/2018 02/02/2018\n",
      "06/01/2018 03/23/2018\n",
      "08/28/2019 08/31/2018\n",
      "07/25/2018 08/11/2018\n",
      "01/22/2020 04/16/2019\n",
      "01/17/2018 12/01/2017\n",
      "01/05/2017 01/29/2017\n",
      "02/02/2017 02/14/2017\n",
      "03/07/2017 04/21/2017\n",
      "06/16/2017 05/05/2017\n",
      "05/15/2017 08/03/2017\n",
      "07/11/2017 08/06/2017\n",
      "07/21/2017 10/22/2016\n",
      "11/27/2017 09/18/2017\n",
      "10/24/2017 11/06/2017\n",
      "01/25/2018 11/02/2017\n",
      "01/23/2018 12/19/2017\n",
      "02/27/2018 04/29/2017\n",
      "02/08/2018 11/30/2017\n",
      "02/08/2018 04/01/2018\n",
      "06/15/2018 03/07/2018\n",
      "03/15/2018 01/17/2018\n",
      "04/27/2018 05/14/2018\n",
      "03/01/2018 03/16/2018\n",
      "06/26/2018 07/01/2018\n",
      "07/24/2018 06/22/2018\n",
      "07/27/2018 06/20/2018\n",
      "11/02/2018 12/08/2018\n",
      "09/11/2018 10/05/2018\n",
      "09/21/2018 10/08/2018\n",
      "01/18/2019 08/19/2011\n",
      "01/23/2019 09/07/2018\n",
      "03/12/2019 11/21/2018\n",
      "11/15/2016 01/20/2017\n",
      "02/01/2017 02/18/2017\n",
      "12/12/2017 08/01/2017\n",
      "05/16/2018 05/18/2018\n",
      "06/20/2017 07/03/2017\n",
      "06/23/2017 06/28/2017\n",
      "02/14/2018 01/02/2018\n",
      "05/30/2018 02/26/2018\n",
      "10/29/2018 12/05/2018\n",
      "12/27/2018 12/17/2018\n",
      "01/22/2020 10/14/2016\n",
      "02/09/2018 02/27/2018\n",
      "02/09/2018 11/01/2017\n",
      "04/09/2018 11/15/2016\n",
      "02/21/2018 01/30/2018\n",
      "05/08/2018 06/26/2018\n",
      "05/23/2018 07/20/2017\n",
      "06/20/2018 03/01/2013\n",
      "07/03/2018 07/31/2017\n",
      "07/18/2018 01/01/2015\n",
      "07/09/2019 07/09/2018\n",
      "09/10/2018 07/15/2008\n",
      "10/05/2018 07/18/2018\n",
      "10/16/2019 08/07/2019\n",
      "08/19/2019 08/08/2019\n",
      "10/24/2019 09/12/2019\n",
      "08/23/2019 02/11/2020\n",
      "03/11/2020 10/11/2019\n",
      "11/27/2019 07/03/2019\n",
      "01/10/2020 09/20/2019\n",
      "06/23/2020 07/03/2020\n",
      "12/05/2019 07/05/2019\n",
      "03/03/2017 02/01/2017\n",
      "03/28/2017 04/11/2016\n",
      "04/27/2017 05/10/2017\n",
      "05/07/2017 03/16/2017\n",
      "04/28/2017 01/01/2017\n",
      "06/02/2017 05/10/2017\n",
      "07/28/2017 07/13/2017\n",
      "03/19/2018 12/05/2017\n",
      "03/19/2018 12/05/2017\n",
      "03/19/2018 12/05/2017\n",
      "02/01/2018 03/22/2018\n",
      "02/05/2018 11/10/2017\n",
      "03/15/2018 01/31/2018\n",
      "01/16/2018 03/09/2018\n",
      "01/03/2018 08/13/2017\n",
      "01/24/2018 01/24/2017\n",
      "01/19/2019 01/19/2018\n",
      "10/23/2018 09/01/2017\n",
      "02/07/2018 04/06/2018\n",
      "04/27/2018 01/09/2018\n",
      "04/03/2018 04/20/2018\n",
      "02/16/2018 02/26/2018\n",
      "03/06/2018 03/16/2018\n",
      "04/30/2018 02/24/2018\n",
      "03/19/2018 02/19/2018\n",
      "04/03/2018 02/02/2018\n",
      "05/09/2018 05/31/2018\n",
      "04/11/2018 05/15/2018\n",
      "07/06/2018 06/06/2018\n",
      "04/03/2018 04/15/2018\n",
      "04/25/2018 05/07/2018\n",
      "04/30/2018 05/25/2018\n",
      "05/10/2018 03/17/2018\n",
      "05/17/2018 03/16/2018\n",
      "05/16/2018 01/01/2018\n",
      "06/11/2018 06/23/2018\n",
      "06/04/2018 06/08/2017\n",
      "05/31/2018 05/30/2018\n",
      "06/11/2018 07/03/2018\n",
      "06/25/2018 07/09/2018\n",
      "11/13/2018 11/16/2018\n",
      "11/13/2018 11/16/2018\n",
      "12/11/2018 11/16/2018\n",
      "01/30/2020 10/04/2018\n",
      "02/27/2018 04/13/2018\n",
      "05/29/2018 06/11/2018\n",
      "07/18/2018 08/03/2018\n",
      "06/04/2018 06/16/2018\n",
      "05/31/2018 06/23/2018\n",
      "07/10/2018 07/24/2018\n",
      "07/09/2018 05/01/2018\n",
      "07/24/2018 08/19/2018\n",
      "07/23/2018 08/06/2018\n",
      "10/16/2018 10/30/2018\n",
      "10/09/2018 10/11/2018\n",
      "10/03/2018 11/08/2018\n",
      "10/18/2018 11/05/2018\n",
      "10/31/2019 12/13/2018\n",
      "12/13/2019 01/26/2020\n",
      "02/18/2020 03/17/2020\n",
      "01/14/2020 12/01/2018\n",
      "01/24/2020 09/01/2018\n",
      "01/13/2020 01/25/2020\n",
      "02/26/2018 10/10/2017\n",
      "01/25/2018 05/11/2013\n",
      "04/16/2019 01/17/2019\n",
      "05/28/2019 10/19/2018\n",
      "08/09/2019 08/30/2019\n",
      "09/12/2019 12/20/2018\n",
      "12/10/2019 05/10/2021\n",
      "08/30/2019 11/11/2019\n",
      "12/09/2019 10/21/2019\n",
      "02/27/2020 02/07/2020\n",
      "03/01/2020 03/02/2020\n",
      "03/20/2018 07/24/2017\n",
      "02/07/2018 11/14/2017\n",
      "06/07/2018 04/11/2018\n",
      "01/15/2018 02/01/2017\n",
      "01/24/2018 02/06/2018\n",
      "02/22/2018 01/29/2018\n",
      "03/02/2018 03/17/2018\n",
      "05/29/2018 04/17/2018\n",
      "06/25/2018 02/07/2018\n",
      "03/22/2018 04/15/2018\n",
      "04/05/2018 03/01/2018\n",
      "03/26/2018 11/30/2017\n",
      "12/04/2019 06/30/2018\n",
      "04/13/2018 03/30/2018\n",
      "05/01/2018 05/10/2018\n",
      "04/23/0004 04/23/2018\n",
      "08/23/2018 07/12/2018\n",
      "06/01/2018 06/19/2018\n",
      "06/14/2018 04/09/2018\n",
      "06/14/2018 07/17/2018\n",
      "01/18/2019 01/28/2019\n",
      "10/04/2018 10/22/2018\n",
      "06/28/2018 07/15/2018\n",
      "06/19/2018 08/17/2018\n",
      "06/19/2018 08/05/2018\n",
      "05/02/2019 05/24/2019\n",
      "08/29/2018 04/30/2018\n",
      "11/27/2018 07/21/2018\n",
      "06/06/2019 10/01/2015\n",
      "10/24/2018 11/05/2018\n",
      "11/02/2018 01/18/2019\n",
      "12/06/2018 11/08/2018\n",
      "11/23/2018 12/10/2018\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(gold_data['hearing_date'], new_df['hearing_date_raw_text']))\n",
    "print(accuracy_score(gold_data['hearing_date'], new_df['hearing_date_cleaned_text']))\n",
    "for gold, pred in zip(gold_data['hearing_date'], new_df['hearing_date_cleaned_text']):\n",
    "    if gold != pred:\n",
    "        print(gold, pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silver Data\n",
    "- Only 678 of 702 case files match"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating `silver_data` df from `gold_data` raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_file_text</th>\n",
       "      <th>raw_file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Metadata:\\nDate:\\t2018-07-06\\nFile number:\\t\\n...</td>\n",
       "      <td>SWL-17348-18.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Metadata:\\nDate:\\t2017-05-26\\nFile number:\\t\\n...</td>\n",
       "      <td>TEL-79722-17.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Metadata:\\nDate:\\t2017-07-05\\nFile number:\\t\\n...</td>\n",
       "      <td>TEL-80773-17.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Metadata:\\nDate:\\t2017-07-18\\nFile number:\\t\\n...</td>\n",
       "      <td>TEL-81359-17-AM.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Metadata:\\nDate:\\t2017-08-17\\nFile number:\\t\\n...</td>\n",
       "      <td>TEL-81405-17.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>Metadata:\\nDate:\\t2018-11-23\\nFile number:\\t\\n...</td>\n",
       "      <td>TSL-99691-18.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>Metadata:\\nDate:\\t2018-11-29\\nFile number:\\t\\n...</td>\n",
       "      <td>TSL-99824-18.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>Metadata:\\nDate:\\t2018-12-12\\nFile number:\\t\\n...</td>\n",
       "      <td>TSL-99900-18.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>Metadata:\\nDate:\\t2018-11-20\\nFile number:\\t\\n...</td>\n",
       "      <td>TSL-99965-18.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>Metadata:\\nDate:\\t2019-09-06\\nFile number:\\t\\n...</td>\n",
       "      <td>TST-06337-19-IN.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>679 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         raw_file_text         raw_file_name\n",
       "0    Metadata:\\nDate:\\t2018-07-06\\nFile number:\\t\\n...      SWL-17348-18.txt\n",
       "1    Metadata:\\nDate:\\t2017-05-26\\nFile number:\\t\\n...      TEL-79722-17.txt\n",
       "2    Metadata:\\nDate:\\t2017-07-05\\nFile number:\\t\\n...      TEL-80773-17.txt\n",
       "3    Metadata:\\nDate:\\t2017-07-18\\nFile number:\\t\\n...   TEL-81359-17-AM.txt\n",
       "4    Metadata:\\nDate:\\t2017-08-17\\nFile number:\\t\\n...      TEL-81405-17.txt\n",
       "..                                                 ...                   ...\n",
       "674  Metadata:\\nDate:\\t2018-11-23\\nFile number:\\t\\n...      TSL-99691-18.txt\n",
       "675  Metadata:\\nDate:\\t2018-11-29\\nFile number:\\t\\n...      TSL-99824-18.txt\n",
       "676  Metadata:\\nDate:\\t2018-12-12\\nFile number:\\t\\n...      TSL-99900-18.txt\n",
       "677  Metadata:\\nDate:\\t2018-11-20\\nFile number:\\t\\n...      TSL-99965-18.txt\n",
       "678  Metadata:\\nDate:\\t2019-09-06\\nFile number:\\t\\n...   TST-06337-19-IN.txt\n",
       "\n",
       "[679 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silver_data = gold_data.copy()\n",
    "silver_data = silver_data.drop(columns = [col for col in silver_data.columns if col not in ['raw_file_name', 'raw_file_text']])\n",
    "silver_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file_number_gold',\n",
       " 'file_number',\n",
       " 'raw_file_text',\n",
       " 'raw_file_name',\n",
       " 'timestamp',\n",
       " 'email_address',\n",
       " 'hearing_date',\n",
       " 'decision_date',\n",
       " 'adjudicating_member',\n",
       " 'ltb_location',\n",
       " 'landlord_represented',\n",
       " 'landlord_attended_hearing',\n",
       " 'tenant_represented',\n",
       " 'tenant_attended_hearing',\n",
       " 'landlord_nonprofit',\n",
       " 'tenant_collecting_subsidy',\n",
       " 'case_outcome',\n",
       " 'tenancy_length',\n",
       " 'monthly_rent',\n",
       " 'rental_deposit',\n",
       " 'rent_after_increase',\n",
       " 'rent_increase_effect_date',\n",
       " 'total_arrears',\n",
       " 'arrears_duration',\n",
       " 'arrears_payment_amount',\n",
       " 'tenant_arrears_history_mentioned',\n",
       " 'tenant_arrears_payment_history_mentioned',\n",
       " 'rent_payments_late_frequency',\n",
       " 'tenant_ability_to_pay_rent',\n",
       " 'tenant_conditions',\n",
       " 'tenant_children_present',\n",
       " 'total_children',\n",
       " 'children_17_or_younger',\n",
       " 'children_13_or_younger',\n",
       " 'children_4_or_younger',\n",
       " 'children_conditions_mentioned',\n",
       " 'conditions_making_moving_burdensome',\n",
       " 'tenant_employed',\n",
       " 'tenant_government_assistance',\n",
       " 'employment_stability_doubts',\n",
       " 'sufficient_income_to_pay_rent',\n",
       " 'total_household_income',\n",
       " 'tenant_job_loss_mentioned',\n",
       " 'tenant_extenuating_circumstances',\n",
       " 'tenant_proposed_payment_plan',\n",
       " 'accepted_proposed_payment_plan',\n",
       " 'payment_plan_length',\n",
       " 'tenant_difficulty_finding_housing',\n",
       " 'applicable_difficulty_reasons',\n",
       " 'tenant_prior_notice_given',\n",
       " 'prior_notice_duration',\n",
       " 'postponement_additional_arrears',\n",
       " 'mentioned_applications',\n",
       " 'validity_of_N4_notice_mentioned',\n",
       " 'additional_details_in_decision',\n",
       " 'executive_review',\n",
       " 'review_status']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_data = gold_data.rename(columns = {'file_number_gold_cleaned': 'file_number',\n",
    "                                        'board_location': 'ltb_location'})\n",
    "gold_data.columns.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `get_nulls()`\n",
    "- for checking df after each addition to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_file_text</th>\n",
       "      <th>raw_file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [raw_file_text, raw_file_name]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_nulls(df, col, return_index = False):\n",
    "    # returns a list of the indices of null values in a column of a dataframe\n",
    "    null_rows = silver_data[silver_data[col].isnull()] # df of all rows with null ltb_location\n",
    "    nulls_inds = null_rows.index.tolist()\n",
    "\n",
    "    if return_index:\n",
    "        return nulls_inds\n",
    "    else:\n",
    "        return null_rows\n",
    "    \n",
    "get_nulls(silver_data, 'raw_file_text', return_index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get `hearing_date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
