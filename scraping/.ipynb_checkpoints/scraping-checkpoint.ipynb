{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22cfc2b2-7d8b-41ab-8bfd-d9ccced161bb",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98602068-fd1d-4f79-b5c4-a0740878419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9fe5e0-48c6-4253-b766-f87b6b2ff9c7",
   "metadata": {},
   "source": [
    "### Directory Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261843e7-a010-4bed-a700-5b18e334030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"html_files/\"\n",
    "formatted_data_dir = \"formatted_cases/\"\n",
    "case_data_path = \"case_data/cases.csv\"\n",
    "scraped_searches_path = \"scraped_searches/\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(formatted_data_dir):\n",
    "    os.makedirs(formatted_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aebe329-5b05-4333-a637-6c8eef66663f",
   "metadata": {},
   "source": [
    "# Unannotated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d413ba1-b2d0-4ea8-85af-e6cd57cd6185",
   "metadata": {},
   "source": [
    "### Get URLs of every case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e47965-240f-4f4f-8686-0fb217d61cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_list = []\n",
    "wait_timeout = 10\n",
    "\n",
    "decisions_by_year_path = \"decisions_by_year/\"\n",
    "driver = webdriver.Chrome()\n",
    "for year in range(2005,2023):\n",
    "    url = \"https://www.canlii.org/en/on/onltb/nav/date/\" + str(year) + \"/\"\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait for the presence of the specific element with class \"name\"\n",
    "    wait = WebDriverWait(driver, wait_timeout)\n",
    "    wait.until(EC.presence_of_element_located((By.ID, \"decisionsListing\")))\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Wait for the \"Show more results\" button to be present within the tbody element\n",
    "    show_more_results_locator = (By.CSS_SELECTOR, \"span.link.showMoreResults\")\n",
    "    wait.until(EC.presence_of_element_located(show_more_results_locator))\n",
    "\n",
    "    while True:\n",
    "        # Check if the \"Show more results\" button is present\n",
    "        show_more_results_button = driver.find_element(*show_more_results_locator)\n",
    "        if not show_more_results_button.is_displayed():\n",
    "            break  # Exit the loop if the button is no longer displayed\n",
    "\n",
    "        # Click the \"Show more results\" button\n",
    "        show_more_results_button.click()\n",
    "\n",
    "        # Wait for the page to load after clicking the button\n",
    "        time.sleep(2)\n",
    "        # wait = WebDriverWait(driver, wait_timeout)\n",
    "        # wait.until(EC.presence_of_element_located((By.XPATH, \"//div[@class='result-item']\")))\n",
    "    \n",
    "    # Get all content, including dynamically generated content\n",
    "    html_content = driver.page_source\n",
    "\n",
    "    # Save the content to a file\n",
    "    with open(decisions_by_year_path + str(year) + \".html\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    print(year, \"saved.\")\n",
    "    \n",
    "driver.quit()    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7903cff5-3fb7-4ed1-8d07-3ac2261ca7e9",
   "metadata": {},
   "source": [
    "### Put URLs into DF by year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811ed821-5e82-4ab8-91bc-6c0042c7c35c",
   "metadata": {},
   "source": [
    "urls_by_year_df = pd.DataFrame(columns=['year', 'case_URL'])\n",
    "\n",
    "for file in os.listdir(decisions_by_year_path):\n",
    "    if os.path.isfile(decisions_by_year_path + file):\n",
    "        # try:\n",
    "        if not file.startswith('.'):\n",
    "            with open(decisions_by_year_path + file) as f:\n",
    "                html = f.read()\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                \n",
    "                decisions_list = soup.find('tbody', id='decisionsListing')\n",
    "                anchor_tags = decisions_list.find_all(\"a\")\n",
    "                for a in anchor_tags:\n",
    "                    case_url = 'https://www.canlii.org/' + a['href']\n",
    "                    data = {'year': [os.path.splitext(file)[0]], 'case_URL': [case_url]}\n",
    "                    urls_by_year_df = urls_by_year_df.append(pd.DataFrame(data), ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2268a099-2cd1-46d9-af79-1e539ce7e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls_by_year_df[urls_by_year_df['year'] == \"2008\"]\n",
    "urls_by_year_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c4240-afd4-4909-8dcf-ba9457deec47",
   "metadata": {},
   "source": [
    "### All URLs to documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1317f75-3ea0-4995-82cd-d95f5cc3351a",
   "metadata": {},
   "source": [
    "Done:\n",
    "- 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5286356b-73b0-4ad3-995d-237c1e0da7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_by_year_df = urls_by_year_df[urls_by_year_df['year'] == \"2022\"]\n",
    "urls_list = urls_by_year_df['case_URL'].tolist()\n",
    "# Directory to save the HTML files\n",
    "output_dir = \"45k_scraped_html_files/\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "driver = webdriver.Chrome()\n",
    "# Loop through the case IDs and URLs\n",
    "for i, url in enumerate(urls_list):\n",
    "    # Generate a file name based on the case ID\n",
    "    filename = f\"case_{i}.html\"\n",
    "    if os.path.isfile(os.path.join(output_dir, filename)):\n",
    "        print(filename, \"already added, skipping...\")\n",
    "    else:\n",
    "        # Navigate to the URL\n",
    "        driver.get(url)\n",
    "\n",
    "        # Get the page source\n",
    "        html_content = driver.page_source\n",
    "\n",
    "        # Save the HTML content to a file\n",
    "        with open(os.path.join(output_dir, filename), \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(html_content)\n",
    "\n",
    "        print(f\"Saved HTML content for case ID {i}.\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe60e55-1e56-4843-b8f4-9f8e791becdb",
   "metadata": {},
   "source": [
    "### Get data from html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec383ab-2cec-4d52-9de0-c061d077f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"45k_scraped_html_files/\"\n",
    "formatted_data_dir = \"45k_formatted_cases/\"\n",
    "\n",
    "for file in os.listdir(output_dir):\n",
    "    try:\n",
    "        if os.path.isfile(output_dir + file) and not file.startswith('.'):\n",
    "            # if os.path.isfile(formatted_data_dir + case_ID + '.txt'):\n",
    "            #     print(file, \"already added, skipping...\")\n",
    "            # else:\n",
    "            print(\"Adding \", file, \"...\")\n",
    "            with open(output_dir + file) as f:\n",
    "                html = f.read()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "            # find metadata\n",
    "            document_meta = soup.find(\"div\", {\"id\": \"documentMeta\"}) \n",
    "            meta_items = document_meta.find_all(\"div\", {\"class\": \"row py-1\"})\n",
    "\n",
    "            case_ID = \"\"\n",
    "            meta_data = []\n",
    "            for meta_item in meta_items:\n",
    "                children_text = []\n",
    "                for x in meta_item.findChildren()[:2]:\n",
    "                    children_text.append(x.text)\n",
    "                child_string = '\\t'.join(children_text)\n",
    "                if \"file number\" in child_string.lower():\n",
    "                    case_ID = child_string.split(\"\\t\")[1].strip()\n",
    "                    # print(case_ID)\n",
    "                meta_data.append(child_string)\n",
    "\n",
    "            # print(meta_data)\n",
    "            # find text\n",
    "            document_body = soup.find(\"div\", {\"class\": \"documentcontent\"}).get_text()\n",
    "\n",
    "            # write to file\n",
    "            with open(formatted_data_dir + case_ID + '.txt', 'w') as file:\n",
    "                file.write('Metadata:\\n')\n",
    "                file.write('\\n'.join(meta_data))\n",
    "                file.write('Content:\\n')\n",
    "                file.write(document_body)\n",
    "    except:\n",
    "        print(\"Error with:\", file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaccc5d0-0581-46f6-9ba3-bf60b4f407a2",
   "metadata": {},
   "source": [
    "# Annotated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f36bed-1698-4555-9a5d-cefd8b203a97",
   "metadata": {},
   "source": [
    "### Get case IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "d3bc81fc-931a-4e22-ade6-aef03bf391d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(case_data_path)\n",
    "case_IDs = df['What is the file number of the case?'].tolist()\n",
    "# case_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b6b446-abb9-4ab3-8aa6-58ad1a710133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Selenium WebDriver \n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "errors_list = []\n",
    "wait_timeout = 10\n",
    "for case_ID in case_IDs:\n",
    "    if os.path.isfile(scraped_searches_path + case_ID + \".html\"):\n",
    "        print(case_ID, \"already added, skipping...\")\n",
    "    else:\n",
    "        try:\n",
    "            url = \"https://canlii.org/en/#search/id=\" + case_ID\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Wait for the presence of the specific element with class \"name\"\n",
    "            wait = WebDriverWait(driver, wait_timeout)\n",
    "            wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"name\")))\n",
    "            \n",
    "            # Get all content, including dynamically generated content\n",
    "            html_content = driver.page_source\n",
    "\n",
    "            # Save the content to a file\n",
    "            with open(scraped_searches_path + case_ID + \".html\", \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(html_content)\n",
    "\n",
    "            print(case_ID, \"saved.\")\n",
    "        except Exception as e:\n",
    "            errors_list.append(case_ID)\n",
    "            print(f\"Error occurred for case ID {case_ID}: {str(e)}\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "2028f928-a686-436f-bdf6-d12776292f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred for case ID TEL-01869-19: 'utf-8' codec can't decode byte 0x80 in position 3131: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "urls_df = pd.DataFrame(columns=['case_ID', 'case_URL'])\n",
    "\n",
    "for file in os.listdir(scraped_searches_path):\n",
    "    if os.path.isfile(scraped_searches_path + file):\n",
    "        try:\n",
    "            with open(scraped_searches_path + file) as f:\n",
    "                html = f.read()\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                # Find the <a> element within the <span> element\n",
    "                a_element = soup.find('span', class_='name').find('a')\n",
    "\n",
    "                # Extract the value of the href attribute\n",
    "                href = a_element['href']\n",
    "                case_url = 'https://www.canlii.org/' + href\n",
    "                # print(case_url)\n",
    "                data = {'case_ID': [os.path.splitext(file)[0]], 'case_URL': [case_url]}\n",
    "                urls_df = urls_df.append(pd.DataFrame(data), ignore_index=True)\n",
    "        except Exception as e:\n",
    "            errors_list.append(case_ID)\n",
    "            print(f\"Error occurred for case ID {case_ID}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "1921ed85-4089-47ce-9985-5c49b6fb7b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls_df.iloc[11, 1]\n",
    "# urls_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286b762a-7960-4999-81ba-54e687d12b45",
   "metadata": {},
   "source": [
    "### Get all HTML files from URLS_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf411aca-cf8f-4bc9-add6-b5548fa085db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import os\n",
    "\n",
    "# Set up Selenium WebDriver (you may need to download and configure the appropriate WebDriver for your browser)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Get the lists of case IDs and URLs\n",
    "ids_list = urls_df['case_ID'].tolist()\n",
    "urls_list = urls_df['case_URL'].tolist()\n",
    "\n",
    "# Directory to save the HTML files\n",
    "output_dir = \"scraped_html_files/\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Loop through the case IDs and URLs\n",
    "for case_id, url in zip(ids_list, urls_list):\n",
    "    # Generate a file name based on the case ID\n",
    "    filename = f\"{case_id}.html\"\n",
    "    if os.path.isfile(os.path.join(output_dir, filename)):\n",
    "        print(filename, \"already added, skipping...\")\n",
    "    else:\n",
    "        # Navigate to the URL\n",
    "        driver.get(url)\n",
    "\n",
    "        # Get the page source\n",
    "        html_content = driver.page_source\n",
    "\n",
    "        # Save the HTML content to a file\n",
    "        with open(os.path.join(output_dir, filename), \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(html_content)\n",
    "\n",
    "        print(f\"Saved HTML content for case ID {case_id}.\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab226c61-bc33-4c1a-afa8-f7a84dd00401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids_list = urls_df['case_ID'].tolist()\n",
    "# urls_list = urls_df['case_URL'].tolist()\n",
    "# output_dir = \"scraped_html_files/\"\n",
    "# # urls_list\n",
    "\n",
    "# for id, url in zip(ids_list, urls_list):\n",
    "#     # Generate a file name based on the index\n",
    "#     filename = f\"{id}.html\"\n",
    "#     response = requests.get(url)\n",
    "#     # Save the HTML content to a file\n",
    "#     with open(os.path.join(output_dir, filename), 'w', encoding='utf-8') as file:\n",
    "#         file.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de24fe2-1412-4252-a334-4da9eaf5d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df366e9-9132-4b92-8919-06c7af2a31aa",
   "metadata": {},
   "source": [
    "### Scraping from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da52cb6-11f4-4305-8ea5-31baba1aeb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"scraped_html_files/\"\n",
    "formatted_data_dir = \"formatted_cases/\"\n",
    "for file in os.listdir(output_dir):\n",
    "    if os.path.isfile(output_dir + file):\n",
    "        if os.path.isfile(formatted_data_dir + os.path.splitext(file)[0] + \".txt\"):\n",
    "            print(file, \"already added, skipping...\")\n",
    "        else:\n",
    "            if not file.startswith('.'):\n",
    "                print(\"Adding \", file, \"...\")\n",
    "                with open(output_dir + file) as f:\n",
    "                    html = f.read()\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "                # find metadata\n",
    "                document_meta = soup.find(\"div\", {\"id\": \"documentMeta\"}) \n",
    "                meta_items = document_meta.find_all(\"div\", {\"class\": \"row py-1\"})\n",
    "                meta_data = []\n",
    "                for meta_item in meta_items:\n",
    "                    meta_data.append('\\t'.join([x.text for x in meta_item.findChildren()]))\n",
    "                # print(meta_data)\n",
    "                # find text\n",
    "                document_body = soup.find(\"div\", {\"class\": \"documentcontent\"}).get_text()\n",
    "\n",
    "                # write to file\n",
    "                with open(formatted_data_dir + os.path.splitext(file)[0] + '.txt', 'w') as file:\n",
    "                    file.write('Metadata:\\n')\n",
    "                    file.write('\\n'.join(meta_data))\n",
    "                    file.write('Content:\\n')\n",
    "                    file.write(document_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9345803-6f66-4013-a556-7f6b81295fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
