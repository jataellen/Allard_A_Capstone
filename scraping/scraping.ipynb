{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22cfc2b2-7d8b-41ab-8bfd-d9ccced161bb",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98602068-fd1d-4f79-b5c4-a0740878419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9fe5e0-48c6-4253-b766-f87b6b2ff9c7",
   "metadata": {},
   "source": [
    "### Directory Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "261843e7-a010-4bed-a700-5b18e334030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"html_files/\"\n",
    "formatted_data_dir = \"formatted_cases/\"\n",
    "case_data_path = \"case_data/cases.csv\"\n",
    "scraped_searches_path = \"scraped_searches/\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(formatted_data_dir):\n",
    "    os.makedirs(formatted_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f36bed-1698-4555-9a5d-cefd8b203a97",
   "metadata": {},
   "source": [
    "### Get case IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d3bc81fc-931a-4e22-ade6-aef03bf391d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(case_data_path)\n",
    "case_IDs = df['What is the file number of the case?'].tolist()\n",
    "# case_IDs = case_IDs[2:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b6b446-abb9-4ab3-8aa6-58ad1a710133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Set up Selenium WebDriver (you may need to download and configure the appropriate WebDriver for your browser)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "errors_list = []\n",
    "wait_timeout = 10\n",
    "for case_ID in case_IDs:\n",
    "    if os.path.isfile(scraped_searches_path + case_ID + \".html\"):\n",
    "        print(case_ID, \"already added, skipping...\")\n",
    "    else:\n",
    "        try:\n",
    "            url = \"https://canlii.org/en/#search/id=\" + case_ID\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Wait for the presence of the specific element with class \"name\"\n",
    "            wait = WebDriverWait(driver, wait_timeout)\n",
    "            wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"name\")))\n",
    "            \n",
    "            # Get all content, including dynamically generated content\n",
    "            html_content = driver.page_source\n",
    "\n",
    "            # Save the content to a file\n",
    "            with open(scraped_searches_path + case_ID + \".html\", \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(html_content)\n",
    "\n",
    "            print(case_ID, \"saved.\")\n",
    "        except Exception as e:\n",
    "            errors_list.append(case_ID)\n",
    "            print(f\"Error occurred for case ID {case_ID}: {str(e)}\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2028f928-a686-436f-bdf6-d12776292f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred for case ID TEL-01869-19: 'utf-8' codec can't decode byte 0x80 in position 3131: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "urls_df = pd.DataFrame(columns=['case_ID', 'case_URL'])\n",
    "\n",
    "for file in os.listdir(scraped_searches_path):\n",
    "    if os.path.isfile(scraped_searches_path + file):\n",
    "        try:\n",
    "            with open(scraped_searches_path + file) as f:\n",
    "                html = f.read()\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                # Find the <a> element within the <span> element\n",
    "                a_element = soup.find('span', class_='name').find('a')\n",
    "\n",
    "                # Extract the value of the href attribute\n",
    "                href = a_element['href']\n",
    "                case_url = 'https://www.canlii.org/' + href\n",
    "                # print(case_url)\n",
    "                data = {'case_ID': [os.path.splitext(file)[0]], 'case_URL': [case_url]}\n",
    "                urls_df = urls_df.append(pd.DataFrame(data), ignore_index=True)\n",
    "        except Exception as e:\n",
    "            errors_list.append(case_ID)\n",
    "            print(f\"Error occurred for case ID {case_ID}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1921ed85-4089-47ce-9985-5c49b6fb7b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.canlii.org//en/on/onltb/doc/2020/2020canlii61102/2020canlii61102.html?resultIndex=1'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_df.iloc[11, 1]\n",
    "# urls_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286b762a-7960-4999-81ba-54e687d12b45",
   "metadata": {},
   "source": [
    "### Get all HTML files from URLS_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf411aca-cf8f-4bc9-add6-b5548fa085db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import os\n",
    "\n",
    "# Set up Selenium WebDriver (you may need to download and configure the appropriate WebDriver for your browser)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Get the lists of case IDs and URLs\n",
    "ids_list = urls_df['case_ID'].tolist()\n",
    "urls_list = urls_df['case_URL'].tolist()\n",
    "\n",
    "# Directory to save the HTML files\n",
    "output_dir = \"scraped_html_files\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Loop through the case IDs and URLs\n",
    "for case_id, url in zip(ids_list, urls_list):\n",
    "    # Generate a file name based on the case ID\n",
    "    filename = f\"{case_id}.html\"\n",
    "    \n",
    "    # Navigate to the URL\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Get the page source\n",
    "    html_content = driver.page_source\n",
    "    \n",
    "    # Save the HTML content to a file\n",
    "    with open(os.path.join(output_dir, filename), \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(html_content)\n",
    "    \n",
    "    print(f\"Saved HTML content for case ID {case_id}.\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab226c61-bc33-4c1a-afa8-f7a84dd00401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids_list = urls_df['case_ID'].tolist()\n",
    "# urls_list = urls_df['case_URL'].tolist()\n",
    "# output_dir = \"scraped_html_files/\"\n",
    "# # urls_list\n",
    "\n",
    "# for id, url in zip(ids_list, urls_list):\n",
    "#     # Generate a file name based on the index\n",
    "#     filename = f\"{id}.html\"\n",
    "#     response = requests.get(url)\n",
    "#     # Save the HTML content to a file\n",
    "#     with open(os.path.join(output_dir, filename), 'w', encoding='utf-8') as file:\n",
    "#         file.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1de24fe2-1412-4252-a334-4da9eaf5d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df366e9-9132-4b92-8919-06c7af2a31aa",
   "metadata": {},
   "source": [
    "### Scraping from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8da52cb6-11f4-4305-8ea5-31baba1aeb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding  CEL-87788-19.html ...\n",
      "TNL-22838-19.html already added, skipping...\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(data_dir):\n",
    "    if os.path.isfile(data_dir + file):\n",
    "        if os.path.isfile(formatted_data_dir + os.path.splitext(file)[0] + \".txt\"):\n",
    "            print(file, \"already added, skipping...\")\n",
    "        else:\n",
    "            print(\"Adding \", file, \"...\")\n",
    "            with open(data_dir + file) as f:\n",
    "                html = f.read()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "            # find metadata\n",
    "            document_meta = soup.find(\"div\", {\"id\": \"documentMeta\"}) \n",
    "            meta_items = document_meta.find_all(\"div\", {\"class\": \"row py-1\"})\n",
    "            meta_data = []\n",
    "            for meta_item in meta_items:\n",
    "                meta_data.append('\\t'.join([x.text for x in meta_item.findChildren()]))\n",
    "            # print(meta_data)\n",
    "            # find text\n",
    "            document_body = soup.find(\"div\", {\"class\": \"documentcontent\"}).get_text()\n",
    "\n",
    "            # write to file\n",
    "            with open(formatted_data_dir + os.path.splitext(file)[0] + '.txt', 'w') as file:\n",
    "                file.write('Metadata:\\n')\n",
    "                file.write('\\n'.join(meta_data))\n",
    "                file.write('Content:\\n')\n",
    "                file.write(document_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9345803-6f66-4013-a556-7f6b81295fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
